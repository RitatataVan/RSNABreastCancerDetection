{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#数据集链接\n#https://www.kaggle.com/competitions/rsna-breast-cancer-detection\n#https://www.kaggle.com/datasets/markwijkhuizen/keras-cv-attention-models\n#https://www.kaggle.com/datasets/christofhenkel/nvidia-dali-nightly-cuda110-1230dev\n#https://www.kaggle.com/datasets/christofhenkel/rsna-bc-pip-requirements\n#https:训练完成的模型","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install -q timm==0.6.5 --no-index --find-links=/kaggle/input/rsna-bc-pip-requirements\n#!pip install -q albumentations==1.2.1 --no-index --find-links=/kaggle/input/rsna-bc-pip-requirements\n!pip install -q pylibjpeg-libjpeg==1.3.1 --no-index --find-links=/kaggle/input/rsna-bc-pip-requirements\n!pip install -q pydicom==2.0.0 --no-index --find-links=/kaggle/input/rsna-bc-pip-requirements\n!pip install -q python-gdcm==3.0.20 --no-index --find-links=/kaggle/input/rsna-bc-pip-requirements\n!pip install -q dicomsdl==0.109.1 --no-index --find-links=/kaggle/input/rsna-bc-pip-requirements\n\n!pip install -q /kaggle/input/nvidia-dali-nightly-cuda110-1230dev/nvidia_dali_nightly_cuda110-1.23.0.dev20230203-7187866-py3-none-manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2023-02-16T03:23:12.912595Z","iopub.execute_input":"2023-02-16T03:23:12.912897Z","iopub.status.idle":"2023-02-16T03:24:59.596328Z","shell.execute_reply.started":"2023-02-16T03:23:12.912871Z","shell.execute_reply":"2023-02-16T03:24:59.595150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install Keras CV Attention Model Pip Package for ConvNextV2 Models\n!pip install --no-deps /kaggle/input/keras-cv-attention-models/keras_cv_attention_models-1.3.9-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-02-16T03:24:59.599076Z","iopub.execute_input":"2023-02-16T03:24:59.599473Z","iopub.status.idle":"2023-02-16T03:25:21.923975Z","shell.execute_reply.started":"2023-02-16T03:24:59.599431Z","shell.execute_reply":"2023-02-16T03:25:21.922671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile dcm_file_read.py    \n#import timm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfrom copy import copy\nimport gc\nimport shutil \n\nimport glob\nfrom scipy.special import expit\n\n#import albumentations as A\nimport cv2\ncv2.setNumThreads(0)\n\nimport dicomsdl\nimport pydicom\nfrom pydicom.filebase import DicomBytesIO\n\nfrom os.path import join\n\nfrom tqdm import tqdm\n\nfrom joblib import Parallel, delayed\nimport multiprocessing as mp\n\nfrom types import SimpleNamespace\nfrom typing import Any, Dict\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\n\n\nimport nvidia.dali.fn as fn\nimport nvidia.dali.types as types\nfrom nvidia.dali import pipeline_def\nfrom nvidia.dali.types import DALIDataType\n\n#we need to patch DALI for Int16 support\n\n\nfrom nvidia.dali.backend import TensorGPU, TensorListGPU\nfrom nvidia.dali.pipeline import Pipeline\nimport nvidia.dali.ops as ops\nfrom nvidia.dali import types\nfrom nvidia.dali.plugin.base_iterator import _DaliBaseIterator\nfrom nvidia.dali.plugin.base_iterator import LastBatchPolicy\nimport torch\nimport torch.utils.dlpack as torch_dlpack\nimport ctypes\nimport numpy as np\nimport torch.nn.functional as F\nimport pydicom\n\nto_torch_type = {\n    types.DALIDataType.FLOAT:   torch.float32,\n    types.DALIDataType.FLOAT64: torch.float64,\n    types.DALIDataType.FLOAT16: torch.float16,\n    types.DALIDataType.UINT8:   torch.uint8,\n    types.DALIDataType.INT8:    torch.int8,\n    types.DALIDataType.UINT16:  torch.int16,\n    types.DALIDataType.INT16:   torch.int16,\n    types.DALIDataType.INT32:   torch.int32,\n    types.DALIDataType.INT64:   torch.int64\n}\n\n\ndef feed_ndarray(dali_tensor, arr, cuda_stream=None):\n    \"\"\"\n    Copy contents of DALI tensor to PyTorch's Tensor.\n\n    Parameters\n    ----------\n    `dali_tensor` : nvidia.dali.backend.TensorCPU or nvidia.dali.backend.TensorGPU\n                    Tensor from which to copy\n    `arr` : torch.Tensor\n            Destination of the copy\n    `cuda_stream` : torch.cuda.Stream, cudaStream_t or any value that can be cast to cudaStream_t.\n                    CUDA stream to be used for the copy\n                    (if not provided, an internal user stream will be selected)\n                    In most cases, using pytorch's current stream is expected (for example,\n                    if we are copying to a tensor allocated with torch.zeros(...))\n    \"\"\"\n    dali_type = to_torch_type[dali_tensor.dtype]\n\n    assert dali_type == arr.dtype, (\"The element type of DALI Tensor/TensorList\"\n                                    \" doesn't match the element type of the target PyTorch Tensor: \"\n                                    \"{} vs {}\".format(dali_type, arr.dtype))\n    assert dali_tensor.shape() == list(arr.size()), \\\n        (\"Shapes do not match: DALI tensor has size {0}, but PyTorch Tensor has size {1}\".\n            format(dali_tensor.shape(), list(arr.size())))\n    cuda_stream = types._raw_cuda_stream(cuda_stream)\n\n    # turn raw int to a c void pointer\n    c_type_pointer = ctypes.c_void_p(arr.data_ptr())\n    if isinstance(dali_tensor, (TensorGPU, TensorListGPU)):\n        stream = None if cuda_stream is None else ctypes.c_void_p(cuda_stream)\n        dali_tensor.copy_to_external(c_type_pointer, stream, non_blocking=True)\n    else:\n        dali_tensor.copy_to_external(c_type_pointer)\n    return arr\n\n\n\n\n\n# Params\n\nCOMP_FOLDER = '/kaggle/input/rsna-breast-cancer-detection/'\nDATA_FOLDER = COMP_FOLDER + 'test_images/'\n\nsample_submission = pd.read_csv(COMP_FOLDER + 'sample_submission.csv')\n\nPUBLIC_RUN = len(sample_submission) == 2\n\nN_CORES = mp.cpu_count()\nMIXED_PRECISION = False\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nRAM_CHECK = False\nDEBUG = False\n\ntest_df = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv')\ntest_df['cancer'] = 0 #dummy value\n\n\nif PUBLIC_RUN is False:\n    RAM_CHECK = False\n    DEBUG = False\n\nif RAM_CHECK is True:\n    test_df = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n    patient_filter = list(sorted((set(test_df.patient_id.unique()))))[:8000]\n    test_df = test_df[test_df.patient_id.isin(patient_filter)]\n    DATA_FOLDER = DATA_FOLDER.replace('test','train')\n\nif DEBUG is True:\n    test_df = test_df.head(100)\n\ntest_df\n\nprint(f'Len df : {len(test_df)}')\ntest_df['patient_id'].nunique()\n\ntest_df[\"fns\"] = test_df['patient_id'].astype(str) + '/' + test_df['image_id'].astype(str) + '.dcm'\n\ndef convert_dicom_to_jpg(file, save_folder=\"\"):\n    patient = file.split('/')[-2]\n    image = file.split('/')[-1][:-4]\n    dcmfile = pydicom.dcmread(file)\n\n    if dcmfile.file_meta.TransferSyntaxUID == '1.2.840.10008.1.2.4.90':\n        with open(file, 'rb') as fp:\n            raw = DicomBytesIO(fp.read())\n            ds = pydicom.dcmread(raw)\n        offset = ds.PixelData.find(b\"\\x00\\x00\\x00\\x0C\")  #<---- the jpeg2000 header info we're looking for\n        hackedbitstream = bytearray()\n        hackedbitstream.extend(ds.PixelData[offset:])\n        with open(save_folder + f\"{patient}_{image}.jpg\", \"wb\") as binary_file:\n            binary_file.write(hackedbitstream)\n            \n    if dcmfile.file_meta.TransferSyntaxUID == '1.2.840.10008.1.2.4.70':\n        with open(file, 'rb') as fp:\n            raw = DicomBytesIO(fp.read())\n            ds = pydicom.dcmread(raw)\n        offset = ds.PixelData.find(b\"\\xff\\xd8\\xff\\xe0\")  #<---- the jpeg lossless header info we're looking for\n        hackedbitstream = bytearray()\n        hackedbitstream.extend(ds.PixelData[offset:])\n        with open(save_folder + f\"{patient}_{image}.jpg\", \"wb\") as binary_file:\n            binary_file.write(hackedbitstream)\n\n            \n@pipeline_def\ndef jpg_decode_pipeline(jpgfiles):\n    jpegs, _ = fn.readers.file(files=jpgfiles)\n    images = fn.experimental.decoders.image(jpegs, device='mixed', output_type=types.ANY_DATA, dtype=DALIDataType.UINT16)\n    return images\n\ndef parse_window_element(elem):\n    if type(elem)==list:\n        return float(elem[0])\n    if type(elem)==str:\n        return float(elem)\n    if type(elem)==float:\n        return elem\n    if type(elem)==pydicom.dataelem.DataElement:\n        try:\n            return float(elem[0])\n        except:\n            return float(elem.value)\n    return None\n\ndef linear_window(data, center, width):\n    lower, upper = center - width // 2, center + width // 2\n    data = torch.clamp(data, min=lower, max=upper)\n    return data \n\ndef process_dicom(img, dicom):\n    try:\n        invert = getattr(dicom, \"PhotometricInterpretation\", None) == \"MONOCHROME1\"\n    except:\n        invert = False\n        \n    center = parse_window_element(dicom[\"WindowCenter\"]) \n    width = parse_window_element(dicom[\"WindowWidth\"])\n        \n    if (center is not None) & (width is not None):\n        img = linear_window(img, center, width)\n\n    img = (img - img.min()) / (img.max() - img.min())\n    if invert:\n        img = 1 - img\n    return img\n\nTARGET_HEIGHT = 1344\nTARGET_WIDTH = 768\nN_CHANNELS = 1\n#INPUT_SHAPE = (TARGET_HEIGHT, TARGET_WIDTH, N_CHANNELS)\nTARGET_HEIGHT_WIDTH_RATIO = TARGET_HEIGHT / TARGET_WIDTH\n\n# Smooth vector used to smoothen sums/stds of axes\ndef smooth(l):\n    # kernel size is 1% of vector\n    kernel_size = int(len(l) * 0.01)\n    kernel = np.ones(kernel_size) / kernel_size\n    return np.convolve(l, kernel, mode='same')\n\n# X Crop offset based on first column with sum below 5% of maximum column sums*std\ndef get_x_offset(image, max_col_sum_ratio_threshold=0.01, debug=None):\n    # Image Dimensions\n    H, W = image.shape[:2]\n    # Percentual margin added to offset\n    margin = int(image.shape[1] * 0.00)\n    # Threshold values based on smoothed sum x std to capture varying intensity columns\n    vv = smooth(image.sum(axis=0).squeeze()) * smooth(image.std(axis=0).squeeze())\n    # Find maximum sum in first 75% of columns\n    vv_argmax = vv[:int(image.shape[1] * 0.75)].argmax()\n    # Threshold value\n    vv_threshold = vv.max() * max_col_sum_ratio_threshold\n    \n    # Find first column after maximum column below threshold value\n    for offset, v in enumerate(vv):\n        # Start searching from vv_argmax\n        if offset < vv_argmax:\n            continue\n        \n        # Column below threshold value found\n        if v < vv_threshold:\n            offset = min(W, offset + margin)\n            break\n            \n\n        \n    return offset\n\n# Y Crop offset based on first bottom and top rows with sum below 10% of maximum row sum*std\ndef get_y_offsets(image, max_row_sum_ratio_threshold=0.01, debug=None):\n    # Image Dimensions\n    H, W = image.shape[:2]\n    # Margin to add to offsets\n    margin = 0\n    # Threshold values based on smoothed sum x std to capture varying intensity columns\n    vv = smooth(image.sum(axis=1).squeeze()) * smooth(image.std(axis=1).squeeze())\n    # Find maximum sum * std row in inter quartile rows\n    vv_argmax = int(image.shape[0] * 0.25) + vv[int(image.shape[0] * 0.25):int(image.shape[0] * 0.75)].argmax()\n    # Threshold value\n    vv_threshold = vv.max() * max_row_sum_ratio_threshold\n    # Default crop offsets\n    offset_bottom = 0\n    offset_top = H\n\n    # Bottom offset, search from argmax to bottom\n    for offset in reversed(range(0, vv_argmax)):\n        v = vv[offset]\n        if v < vv_threshold:\n            offset_bottom = offset\n            break\n    \n\n            \n    # Top offset, search from argmax to top\n    for offset in range(vv_argmax, H):\n        v = vv[offset]\n        if v < vv_threshold:\n            offset_top = offset\n            break\n            \n\n            \n    return max(0, offset_bottom - margin), min(image.shape[0], offset_top + margin)\n\n# Crop image and pad offsets to target image height/width ratio to preserve information\ndef crop(image, size=None, debug=False):\n    # Image dimensions\n    H, W = image.shape[:2]\n    # Compute x/bottom/top offsets\n    x_offset = get_x_offset(image, debug=debug)\n    offset_bottom, offset_top = get_y_offsets(image[:,:x_offset], debug=debug)\n    # Crop Height and Width\n    h_crop = offset_top - offset_bottom\n    w_crop = x_offset\n    \n    # Pad crop offsets to target aspect ratio\n    if size is not None:\n        # Height too large, pad x offset\n        if (h_crop / w_crop) > TARGET_HEIGHT_WIDTH_RATIO:\n            x_offset += int(h_crop / TARGET_HEIGHT_WIDTH_RATIO - w_crop)\n        else:\n            # Height too small, pad bottom/top offsets\n            offset_bottom -= int(0.50 * (w_crop * TARGET_HEIGHT_WIDTH_RATIO - h_crop))\n            offset_bottom_correction = max(0, -offset_bottom)\n            offset_bottom += offset_bottom_correction\n\n            offset_top += int(0.50 * (w_crop * TARGET_HEIGHT_WIDTH_RATIO - h_crop))\n            offset_top += offset_bottom_correction\n        \n    # Crop Image\n    image = image[offset_bottom:offset_top:,:x_offset]\n        \n    return image\n\ncfg = SimpleNamespace(**{})\n#cfg.img_size = 1024\n#cfg.backbone = 'seresnext50_32x4d'\ncfg.pretrained=False\ncfg.in_channels = 1\ncfg.classes = ['cancer']\ncfg.batch_size = 8\ncfg.data_folder = \"/kaggle/tmp/output\"\n#cfg.val_aug = A.CenterCrop(always_apply=False, p=1.0, height=cfg.img_size, width=cfg.img_size)\ncfg.device = DEVICE\n\n#SAVE_SIZE = (768,1344)#int(cfg.img_size * 1.125)\nSAVE_FOLDER = cfg.data_folder\nos.makedirs(SAVE_FOLDER, exist_ok=True)\nN_CHUNKS = len(test_df[\"fns\"]) // 2000 if len(test_df[\"fns\"]) > 2000 else 1\nCHUNKS = [(len(test_df[\"fns\"]) / N_CHUNKS * k, len(test_df[\"fns\"]) / N_CHUNKS * (k + 1)) for k in range(N_CHUNKS)]\nCHUNKS = np.array(CHUNKS).astype(int)\nJPG_FOLDER = \"/tmp/jpg/\"\n\n\n\nfor ttt, chunk in enumerate(CHUNKS):\n    print(f'chunk {ttt} of {len(CHUNKS)} chunks')\n    os.makedirs(JPG_FOLDER, exist_ok=True)\n\n    _ = Parallel(n_jobs=2)(\n        delayed(convert_dicom_to_jpg)(f'{DATA_FOLDER}/{img}', save_folder=JPG_FOLDER)\n        for img in test_df[\"fns\"].tolist()[chunk[0]: chunk[1]]\n    )\n    \n    jpgfiles = glob.glob(JPG_FOLDER + \"*.jpg\")\n\n\n    pipe = jpg_decode_pipeline(jpgfiles, batch_size=1, num_threads=2, device_id=0)\n    pipe.build()\n\n    for i, f in enumerate(tqdm(jpgfiles)):\n        \n        patient, dicom_id = f.split('/')[-1][:-4].split('_')\n        dicom = pydicom.dcmread(DATA_FOLDER + f\"/{patient}/{dicom_id}.dcm\")\n        try:\n            out = pipe.run()\n            # Dali -> Torch\n            img = out[0][0]\n            img_torch = torch.empty(img.shape(), dtype=torch.int16, device=\"cuda\")\n            feed_ndarray(img, img_torch, cuda_stream=torch.cuda.current_stream(device=0))\n            img = img_torch.float()\n            #apply dicom preprocessing\n            img = process_dicom(img, dicom)\n\n            #resize the torch image\n            #img = F.interpolate(img.view(1, 1, img.size(0), img.size(1)), (SAVE_SIZE, SAVE_SIZE), mode=\"bilinear\")[0, 0]\n\n            img = (img * 255).clip(0,255).to(torch.uint8).cpu().numpy()[:, :, 0]\n            \n            #out_file_name = SAVE_FOLDER + f\"{patient}_{dicom_id}.png\"\n            #cv2.imwrite(out_file_name, img)\n            h0, w0 = img.shape[:2]\n            if img[:,int(-w0 * 0.10):].sum() > img[:,:int(w0 * 0.10)].sum():\n                img = np.flip(img, axis=1)\n            #img = crop(img, debug=False)  \n            img = crop(img, size=(TARGET_WIDTH, TARGET_HEIGHT), debug=False)\n            h, w = img.shape[:2]\n            if (h / w) > TARGET_HEIGHT_WIDTH_RATIO:\n                pad = int(h / TARGET_HEIGHT_WIDTH_RATIO - w)\n                img = np.pad(img, [[0,0], [0, pad]])\n                h, w = img.shape[:2]\n            else:\n                pad = int(0.50 * (w * TARGET_HEIGHT_WIDTH_RATIO - h))\n                img = np.pad(img, [[pad, pad], [0,0]])\n                h, w = img.shape[:2]\n            # Resize\n            img = cv2.resize(img, (TARGET_WIDTH, TARGET_HEIGHT), interpolation=cv2.INTER_AREA)\n            cv2.imwrite(f'{SAVE_FOLDER}/{dicom_id}.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 100])\n    \n        except Exception as e:\n            print(i, e)\n            pipe = jpg_decode_pipeline(jpgfiles[i+1:], batch_size=1, num_threads=2, device_id=0)\n            pipe.build()\n            continue\n\n    shutil.rmtree(JPG_FOLDER)\nprint(f'DALI Raw image load complete')\n\nfns = glob.glob(f'{SAVE_FOLDER}/*.jpg')\nn_saved = len(fns)\nprint(f'Image on disk count : {n_saved}')\n\ngpu_processed_files = [fn.split('/')[-1].replace('jpg','dcm') for fn in fns]\n\n\n\nto_process = [f for f in test_df[\"fns\"].values if f.split('/')[-1] not in gpu_processed_files]\nlen(gpu_processed_files), len(to_process)\n\n\ndef process(f, save_folder=\"\"):\n    patient = f.split('/')[-2]\n    dicom_id = f.split('/')[-1][:-4]\n    \n    dicom = dicomsdl.open(f)\n    img = dicom.pixelData()\n    img = torch.from_numpy(img)\n    img = process_dicom(img, dicom)\n    \n    #img = F.interpolate(img.view(1, 1, img.size(0), img.size(1)), (SAVE_SIZE, SAVE_SIZE), mode=\"bilinear\")[0, 0]\n\n    img = (img * 255).clip(0,255).to(torch.uint8).cpu().numpy()\n    \n    \n    #cv2.imwrite(out_file_name, img)\n    h0, w0 = img.shape[:2]\n    if img[:,int(-w0 * 0.10):].sum() > img[:,:int(w0 * 0.10)].sum():\n        img = np.flip(img, axis=1)\n    #img = crop(img, debug=False)  \n    img = crop(img, size=(TARGET_WIDTH, TARGET_HEIGHT), debug=False)\n    h, w = img.shape[:2]\n    if (h / w) > TARGET_HEIGHT_WIDTH_RATIO:\n        pad = int(h / TARGET_HEIGHT_WIDTH_RATIO - w)\n        img = np.pad(img, [[0,0], [0, pad]])\n        h, w = img.shape[:2]\n    else:\n        pad = int(0.50 * (w * TARGET_HEIGHT_WIDTH_RATIO - h))\n        img = np.pad(img, [[pad, pad], [0,0]])\n        h, w = img.shape[:2]\n    # Resize\n    img = cv2.resize(img, (TARGET_WIDTH, TARGET_HEIGHT), interpolation=cv2.INTER_AREA)\n    cv2.imwrite(f'{SAVE_FOLDER}/{dicom_id}.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 100])\n    out_file_name = f'{SAVE_FOLDER}/{dicom_id}.jpg'\n    return out_file_name\n\ncpu_processed_filenames = Parallel(n_jobs=2)(\n    delayed(process)(f'{DATA_FOLDER}/{img}', save_folder=SAVE_FOLDER)\n    for img in tqdm(to_process)\n)\ncpu_processed_filenames = [f for f in cpu_processed_filenames if f]\nprint(f'CPU Raw image load complete with {len(cpu_processed_filenames)} loaded')\n\ngc.collect()\ntorch.cuda.empty_cache()\n\nn_saved = len(glob.glob(f'{SAVE_FOLDER}/*.jpg'))\nprint(f'Image on disk count : {n_saved}')\n\nassert n_saved == len(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-02-16T03:25:21.927947Z","iopub.execute_input":"2023-02-16T03:25:21.928647Z","iopub.status.idle":"2023-02-16T03:25:21.949343Z","shell.execute_reply.started":"2023-02-16T03:25:21.928600Z","shell.execute_reply":"2023-02-16T03:25:21.948007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python dcm_file_read.py","metadata":{"execution":{"iopub.status.busy":"2023-02-16T03:25:21.951342Z","iopub.execute_input":"2023-02-16T03:25:21.951860Z","iopub.status.idle":"2023-02-16T03:25:36.773169Z","shell.execute_reply.started":"2023-02-16T03:25:21.951821Z","shell.execute_reply":"2023-02-16T03:25:36.771832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile model1.py\nfrom numba import cuda\nimport torch\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras_cv_attention_models import convnext\nimport cv2\nimport glob\nimport os\nimport time\nimport gc\nfrom scipy.stats import rankdata\nTARGET_HEIGHT = 1344\nTARGET_WIDTH = 768\nN_CHANNELS = 3\nINPUT_SHAPE = (TARGET_HEIGHT, TARGET_WIDTH, N_CHANNELS)\nTARGET_HEIGHT_WIDTH_RATIO = TARGET_HEIGHT / TARGET_WIDTH\n\ntest = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv', usecols=['patient_id', 'image_id','laterality'])\ntest_paths = ('/kaggle/tmp/output/'+test['image_id'].map(str)+'.jpg').tolist()\ndef decode(path):\n    file_bytes = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(file_bytes, channels=3)\n    img = tf.cast(img, tf.float32) \n    return img\ndef build_dataset(slices, bsize=4, decode_fn=None):\n    AUTO = tf.data.experimental.AUTOTUNE\n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.batch(bsize).prefetch(AUTO)\n    return dset\ndtest = build_dataset(test_paths, bsize=8,  decode_fn=decode)\ndef get_model():\n    image = tf.keras.layers.Input(INPUT_SHAPE, name='image', dtype=tf.float32)\n    image_norm = tf.keras.applications.imagenet_utils.preprocess_input(image, mode='torch')\n    x = convnext.ConvNeXtV2Tiny(\n        input_shape=(TARGET_HEIGHT, TARGET_WIDTH, 3),\n        pretrained=None,\n        num_classes=0,\n    )(image_norm)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.models.Model(inputs=image, outputs=outputs)\n    model.trainable = False\n    model.compile()\n\n    return model\n\ntf.keras.backend.clear_session()\ntf.config.optimizer.set_jit(True)\nmodel_paths = ['/kaggle/input/rsna-trained-models/convnextv2_768/model_convext_exm4.h5',\n              '/kaggle/input/rsna-trained-models/convnextv2_768/model_convext_exm3.h5',\n              '/kaggle/input/rsna-trained-models/convnextv2_768/model_convext_exm2.h5',\n              '/kaggle/input/rsna-trained-models/convnextv2_768/model_convext_exm1.h5',\n              ]\nwith tf.device('/device:GPU:0'):\n    models = []\n    for p in model_paths:\n        model = get_model()\n        model.load_weights(p)\n        models.append(model)\n\ntest['cancer'] = sum([model.predict(dtest, verbose=1) for model in models]) / len(models)\nsub = test.groupby(['patient_id', 'laterality'])['cancer'].agg(['mean']).reset_index().rename(columns={'mean':'cancer'})\nsub['prediction_id'] = sub['patient_id'].map(str)+'_'+sub['laterality'].map(str)\nsub[['prediction_id', 'cancer']].to_csv('sub1.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile model2.py\nfrom numba import cuda\nimport torch\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras_cv_attention_models import convnext\nimport cv2\nimport glob\nimport os\nimport time\nimport gc\nfrom scipy.stats import rankdata\nTARGET_HEIGHT = 1344\nTARGET_WIDTH = 768\nN_CHANNELS = 3\nINPUT_SHAPE = (TARGET_HEIGHT, TARGET_WIDTH, N_CHANNELS)\nTARGET_HEIGHT_WIDTH_RATIO = TARGET_HEIGHT / TARGET_WIDTH\n\ntest = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv', usecols=['patient_id', 'image_id','laterality'])\ntest_paths = ('/kaggle/tmp/output/'+test['image_id'].map(str)+'.jpg').tolist()\ndef decode(path):\n    file_bytes = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(file_bytes, channels=3)\n    img = tf.cast(img, tf.float32) \n    return img\ndef build_dataset(slices, bsize=4, decode_fn=None):\n    AUTO = tf.data.experimental.AUTOTUNE\n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.batch(bsize).prefetch(AUTO)\n    return dset\ndtest = build_dataset(test_paths, bsize=8,  decode_fn=decode)\ndef get_model():\n    image = tf.keras.layers.Input(INPUT_SHAPE, name='image', dtype=tf.float32)\n    image_norm = tf.keras.applications.imagenet_utils.preprocess_input(image, mode='torch')\n    x = convnext.ConvNeXtV2Tiny(\n        input_shape=(TARGET_HEIGHT, TARGET_WIDTH, 3),\n        pretrained=None,\n        num_classes=0,\n    )(image_norm)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.models.Model(inputs=image, outputs=outputs)\n    model.trainable = False\n    model.compile()\n\n    return model\n\ntf.keras.backend.clear_session()\ntf.config.optimizer.set_jit(True)\nmodel_paths = [\n    '/kaggle/input/rsna-trained-models/convnextv2_768/model_extra_all_f0.h5',\n    '/kaggle/input/rsna-trained-models/convnextv2_768/model_extra_all_f1.h5',\n    '/kaggle/input/rsna-trained-models/convnextv2_768/model_extra_all_f2.h5',\n    '/kaggle/input/rsna-trained-models/convnextv2_768/model_extra_all_f4.h5',\n              ]\nwith tf.device('/device:GPU:0'):\n    models = []\n    for p in model_paths:\n        model = get_model()\n        model.load_weights(p)\n        models.append(model)\n\ntest['cancer'] = sum([model.predict(dtest, verbose=1) for model in models]) / len(models)\nsub = test.groupby(['patient_id', 'laterality'])['cancer'].agg(['mean']).reset_index().rename(columns={'mean':'cancer'})\nsub['prediction_id'] = sub['patient_id'].map(str)+'_'+sub['laterality'].map(str)\nsub[['prediction_id', 'cancer']].to_csv('sub2.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python model1.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python model2.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nsub1 = pd.read_csv('sub1.csv').sort_values(['prediction_id']).reset_index(drop=True)\nsub2 = pd.read_csv('sub2.csv').sort_values(['prediction_id']).reset_index(drop=True)\n\nsub1['cancer'] = sub1['cancer']+sub2['cancer']\n\nth = np.quantile(sub1['cancer'].values,0.98)#1.0-(1158+100)/53548\nsub1['cancer'] = (sub1['cancer'].values > th).astype(int)\n\n# Save submission as CSV\nsub1.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}