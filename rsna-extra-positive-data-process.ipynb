{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#数据集链接\n#https://www.kaggle.com/competitions/rsna-breast-cancer-detection\n#https://www.kaggle.com/datasets/pourchot/ddsm-mammography-positive-case\n#https://www.kaggle.com/datasets/remekkinas/rsnamodules\n#https://www.kaggle.com/code/vslaykovsky/rsna-2022-whl","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# Source: https://www.kaggle.com/code/remekkinas/fast-dicom-processing-1-6-2x-faster?scriptVersionId=113360473\n!pip install /kaggle/input/rsnamodules/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl \n\ntry:\n    import pylibjpeg\nexcept:\n   !pip install /kaggle/input/rsna-2022-whl/{pylibjpeg-1.4.0-py3-none-any.whl,python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl}","metadata":{"papermill":{"duration":60.026435,"end_time":"2023-01-30T03:36:29.450678","exception":false,"start_time":"2023-01-30T03:35:29.424243","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-08T05:07:54.293014Z","iopub.execute_input":"2023-02-08T05:07:54.293657Z","iopub.status.idle":"2023-02-08T05:08:19.408047Z","shell.execute_reply.started":"2023-02-08T05:07:54.293527Z","shell.execute_reply":"2023-02-08T05:08:19.406360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pylibjpeg\nimport pydicom\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\nfrom multiprocessing import cpu_count\n\nimport cv2\nimport glob\nimport importlib\nimport os\nimport joblib\nimport time\nimport dicomsdl\nimport gc\n\n# Tensorflow and CV2 set number of threads to 1 for speedup in parallell function mapping\ntf.config.threading.set_inter_op_parallelism_threads(num_threads=1)\ncv2.setNumThreads(1)\n\n# Pandas DataFrame Display Options\npd.options.display.max_colwidth = 99","metadata":{"papermill":{"duration":5.989409,"end_time":"2023-01-30T03:36:57.793249","exception":false,"start_time":"2023-01-30T03:36:51.803840","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-08T05:08:19.410921Z","iopub.execute_input":"2023-02-08T05:08:19.411997Z","iopub.status.idle":"2023-02-08T05:08:25.508452Z","shell.execute_reply.started":"2023-02-08T05:08:19.411941Z","shell.execute_reply":"2023-02-08T05:08:25.507264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath = sorted(glob.glob('/kaggle/input/ddsm-mammography-positive-case/DICOM/DICOM/*/*.dcm'))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:25.509842Z","iopub.execute_input":"2023-02-08T05:08:25.510965Z","iopub.status.idle":"2023-02-08T05:08:31.132701Z","shell.execute_reply.started":"2023-02-08T05:08:25.510921Z","shell.execute_reply":"2023-02-08T05:08:31.130920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/ddsm-mammography-positive-case/additional_train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:31.136402Z","iopub.execute_input":"2023-02-08T05:08:31.136917Z","iopub.status.idle":"2023-02-08T05:08:31.165125Z","shell.execute_reply.started":"2023-02-08T05:08:31.136868Z","shell.execute_reply":"2023-02-08T05:08:31.164063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.sort_values(['patient_id']).reset_index(drop=True)\ntrain['file_path'] = filepath","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:31.166505Z","iopub.execute_input":"2023-02-08T05:08:31.167531Z","iopub.status.idle":"2023-02-08T05:08:31.184907Z","shell.execute_reply.started":"2023-02-08T05:08:31.167483Z","shell.execute_reply":"2023-02-08T05:08:31.183844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(train['patient_id'].map(lambda x:x.split('_')[-1])==train['file_path'].map(lambda x:x.split('/')[-2].split('_')[1]))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:31.186604Z","iopub.execute_input":"2023-02-08T05:08:31.187344Z","iopub.status.idle":"2023-02-08T05:08:31.202691Z","shell.execute_reply.started":"2023-02-08T05:08:31.187302Z","shell.execute_reply":"2023-02-08T05:08:31.201670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['image_id'] = train['file_path'].map(lambda x:x.split('/')[-2].split('_')[-1])","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:31.204076Z","iopub.execute_input":"2023-02-08T05:08:31.204461Z","iopub.status.idle":"2023-02-08T05:08:31.218692Z","shell.execute_reply.started":"2023-02-08T05:08:31.204417Z","shell.execute_reply":"2023-02-08T05:08:31.217650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:31.219875Z","iopub.execute_input":"2023-02-08T05:08:31.220724Z","iopub.status.idle":"2023-02-08T05:08:31.406508Z","shell.execute_reply.started":"2023-02-08T05:08:31.220687Z","shell.execute_reply":"2023-02-08T05:08:31.405209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{"papermill":{"duration":0.006026,"end_time":"2023-01-30T03:36:57.806018","exception":false,"start_time":"2023-01-30T03:36:57.799992","status":"completed"},"tags":[]}},{"cell_type":"code","source":"IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n\nTARGET_HEIGHT = 1344\nTARGET_WIDTH = 768\nN_CHANNELS = 1\nINPUT_SHAPE = (TARGET_HEIGHT, TARGET_WIDTH, N_CHANNELS)\nTARGET_HEIGHT_WIDTH_RATIO = TARGET_HEIGHT / TARGET_WIDTH\nTHRESHOLD_BEST = 0.50\n\nCLAHE = cv2.createCLAHE(clipLimit=0.6, tileGridSize=(16, 16))\n\nCROP_IMAGE = True\nAPPLY_CLAHE = True\nAPPLY_EQ_HIST = False\n\nIMAGE_FORMAT = 'jpg'\nIMAGE_QUALITY = 95","metadata":{"papermill":{"duration":0.017813,"end_time":"2023-01-30T03:36:57.831568","exception":false,"start_time":"2023-01-30T03:36:57.813755","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-08T05:08:31.407765Z","iopub.execute_input":"2023-02-08T05:08:31.408129Z","iopub.status.idle":"2023-02-08T05:08:31.417916Z","shell.execute_reply.started":"2023-02-08T05:08:31.408097Z","shell.execute_reply":"2023-02-08T05:08:31.416898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VOI LUT","metadata":{"papermill":{"duration":0.005941,"end_time":"2023-01-30T03:36:57.843633","exception":false,"start_time":"2023-01-30T03:36:57.837692","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Source: https://www.kaggle.com/code/bobdegraaf/dicomsdl-voi-lut\ndef voi_lut(image, dicom):\n    # Additional Checks\n    if 'WindowWidth' not in dicom.getPixelDataInfo() or 'WindowWidth' not in dicom.getPixelDataInfo():\n        return image\n    \n    # Load only the variables we need\n    center = dicom['WindowCenter']\n    width = dicom['WindowWidth']\n    bits_stored = dicom['BitsStored']\n    voi_lut_function = dicom['VOILUTFunction']\n\n    # For sigmoid it's a list, otherwise a single value\n    if isinstance(center, list):\n        center = center[0]\n    if isinstance(width, list):\n        width = width[0]\n\n    # Set y_min, max & range\n    y_min = 0\n    y_max = float(2**bits_stored - 1)\n    y_range = y_max\n\n    # Function with default LINEAR (so for Nan, it will use linear)\n    if voi_lut_function == 'SIGMOID':\n        image = y_range / (1 + np.exp(-4 * (image - center) / width)) + y_min\n    else:\n        # Checks width for < 1 (in our case not necessary, always >= 750)\n        center -= 0.5\n        width -= 1\n\n        below = image <= (center - width / 2)\n        above = image > (center + width / 2)\n        between = np.logical_and(~below, ~above)\n\n        image[below] = y_min\n        image[above] = y_max\n        if between.any():\n            image[between] = (\n                ((image[between] - center) / width + 0.5) * y_range + y_min\n            )\n\n    return image","metadata":{"papermill":{"duration":0.017649,"end_time":"2023-01-30T03:36:57.867426","exception":false,"start_time":"2023-01-30T03:36:57.849777","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-08T05:08:31.421687Z","iopub.execute_input":"2023-02-08T05:08:31.422322Z","iopub.status.idle":"2023-02-08T05:08:31.432661Z","shell.execute_reply.started":"2023-02-08T05:08:31.422283Z","shell.execute_reply":"2023-02-08T05:08:31.431692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Crop Image","metadata":{"papermill":{"duration":0.006033,"end_time":"2023-01-30T03:36:57.879586","exception":false,"start_time":"2023-01-30T03:36:57.873553","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Smooth vector used to smoothen sums/stds of axes\ndef smooth(l):\n    # kernel size is 1% of vector\n    kernel_size = int(len(l) * 0.01)\n    kernel = np.ones(kernel_size) / kernel_size\n    return np.convolve(l, kernel, mode='same')\n\n# X Crop offset based on first column with sum below 5% of maximum column sums*std\ndef get_x_offset(image, max_col_sum_ratio_threshold=0.05, debug=None):\n    # Image Dimensions\n    H, W = image.shape\n    # Percentual margin added to offset\n    margin = int(image.shape[1] * 0.00)\n    # Threshold values based on smoothed sum x std to capture varying intensity columns\n    vv = smooth(image.sum(axis=0).squeeze()) * smooth(image.std(axis=0).squeeze())\n    # Find maximum sum in first 75% of columns\n    vv_argmax = vv[:int(image.shape[1] * 0.75)].argmax()\n    # Threshold value\n    vv_threshold = vv.max() * max_col_sum_ratio_threshold\n    \n    # Find first column after maximum column below threshold value\n    for offset, v in enumerate(vv):\n        # Start searching from vv_argmax\n        if offset < vv_argmax:\n            continue\n        \n        # Column below threshold value found\n        if v < vv_threshold:\n            offset = min(W, offset + margin)\n            break\n            \n    if isinstance(debug, np.ndarray):\n        debug[1].imshow(image)\n        debug[1].set_title('X Offset')\n        vv_scale = H / vv.max() * 0.90\n        # Values\n        debug[1].plot(H - vv * vv_scale , c='red', label='vv')\n        # Threshold\n        debug[1].hlines(H - vv_threshold * vv_scale, 0, W -1, colors='orange', label='threshold')\n        # Max Value\n        debug[1].scatter(vv_argmax, H - vv[vv_argmax] * vv_scale, c='blue', s=100, label='Max', zorder=np.PINF)\n        # First Column Below Threshold\n        debug[1].scatter(offset, H - vv[offset] * vv_scale, c='purple', s=100, label='Offset', zorder=np.PINF)\n        debug[1].set_ylim(H, 0)\n        debug[1].legend()\n        debug[1].axis('off')\n        \n    return offset\n\n# Y Crop offset based on first bottom and top rows with sum below 10% of maximum row sum*std\ndef get_y_offsets(image, max_row_sum_ratio_threshold=0.05, debug=None):\n    # Image Dimensions\n    H, W = image.shape\n    # Margin to add to offsets\n    margin = 0\n    # Threshold values based on smoothed sum x std to capture varying intensity columns\n    vv = smooth(image.sum(axis=1).squeeze()) * smooth(image.std(axis=1).squeeze())\n    # Find maximum sum * std row in inter quartile rows\n    vv_argmax = int(image.shape[0] * 0.25) + vv[int(image.shape[0] * 0.25):int(image.shape[0] * 0.75)].argmax()\n    # Threshold value\n    vv_threshold = vv.max() * max_row_sum_ratio_threshold\n    # Default crop offsets\n    offset_bottom = 0\n    offset_top = H\n\n    # Bottom offset, search from argmax to bottom\n    for offset in reversed(range(0, vv_argmax)):\n        v = vv[offset]\n        if v < vv_threshold:\n            offset_bottom = offset\n            break\n    \n    if isinstance(debug, np.ndarray):\n        debug[2].imshow(image)\n        debug[2].set_title('Y Bottom Offset')\n        vv_scale = W / vv.max() * 0.90\n        # Values\n        debug[2].plot(vv * vv_scale, np.arange(H), c='red', label='vv')\n        # Threshold\n        debug[2].vlines(vv_threshold * vv_scale, 0, H -1, colors='orange', label='threshold')\n        # Max Value\n        debug[2].scatter(vv[vv_argmax] * vv_scale, vv_argmax, c='blue', s=100, label='Max', zorder=np.PINF)\n        # First Column Below Threshold\n        debug[2].scatter(vv[offset_bottom] * vv_scale, offset_bottom, c='purple', s=100, label='Offset', zorder=np.PINF)\n        debug[2].set_ylim(H, 0)\n        debug[2].legend()\n        debug[2].axis('off')\n            \n    # Top offset, search from argmax to top\n    for offset in range(vv_argmax, H):\n        v = vv[offset]\n        if v < vv_threshold:\n            offset_top = offset\n            break\n            \n    if isinstance(debug, np.ndarray):\n        debug[3].imshow(image)\n        debug[3].set_title('Y Top Offset')\n        vv_scale = W / vv.max() * 0.90\n        # Values\n        debug[3].plot(vv * vv_scale, np.arange(H) , c='red', label='vv')\n        # Threshold\n        debug[3].vlines(vv_threshold * vv_scale, 0, H -1, colors='orange', label='threshold')\n        # Max Value\n        debug[3].scatter(vv[vv_argmax] * vv_scale, vv_argmax, c='blue', s=100, label='Max', zorder=np.PINF)\n        # First Column Below Threshold\n        debug[3].scatter(vv[offset_top] * vv_scale, offset_top, c='purple', s=100, label='Offset', zorder=np.PINF)\n        debug[2].set_ylim(H, 0)\n        debug[3].legend()\n        debug[3].axis('off')\n            \n    return max(0, offset_bottom - margin), min(image.shape[0], offset_top + margin)\n\n# Crop image and pad offsets to target image height/width ratio to preserve information\ndef crop(image, size=None, debug=False):\n    # Image dimensions\n    H, W = image.shape\n    # Compute x/bottom/top offsets\n    x_offset = get_x_offset(image, debug=debug)\n    offset_bottom, offset_top = get_y_offsets(image[:,:x_offset], debug=debug)\n    # Crop Height and Width\n    h_crop = offset_top - offset_bottom\n    w_crop = x_offset\n    \n    # Pad crop offsets to target aspect ratio\n    if size is not None:\n        # Height too large, pad x offset\n        if (h_crop / w_crop) > TARGET_HEIGHT_WIDTH_RATIO:\n            x_offset += int(h_crop / TARGET_HEIGHT_WIDTH_RATIO - w_crop)\n        else:\n            # Height too small, pad bottom/top offsets\n            offset_bottom -= int(0.50 * (w_crop * TARGET_HEIGHT_WIDTH_RATIO - h_crop))\n            offset_bottom_correction = max(0, -offset_bottom)\n            offset_bottom += offset_bottom_correction\n\n            offset_top += int(0.50 * (w_crop * TARGET_HEIGHT_WIDTH_RATIO - h_crop))\n            offset_top += offset_bottom_correction\n        \n    # Crop Image\n    image = image[offset_bottom:offset_top:,:x_offset]\n        \n    return image","metadata":{"papermill":{"duration":0.034876,"end_time":"2023-01-30T03:36:57.920608","exception":false,"start_time":"2023-01-30T03:36:57.885732","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-08T05:08:31.434572Z","iopub.execute_input":"2023-02-08T05:08:31.435411Z","iopub.status.idle":"2023-02-08T05:08:31.468593Z","shell.execute_reply.started":"2023-02-08T05:08:31.435336Z","shell.execute_reply":"2023-02-08T05:08:31.467264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process Image","metadata":{"papermill":{"duration":0.005918,"end_time":"2023-01-30T03:36:57.932570","exception":false,"start_time":"2023-01-30T03:36:57.926652","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# based on: https://www.kaggle.com/code/remekkinas/fast-dicom-processing-1-6-2x-faster?scriptVersionId=113360473\ndef process(file_path, size=None, dicom_process=True, ret_target=False, crop_image=False, apply_clahe=APPLY_CLAHE, apply_eq_hist=APPLY_EQ_HIST, debug=False):\n    # Read Dicom File\n    dicom = dicomsdl.open(file_path)\n    image = dicom.pixelData()\n    \n    # Save original image for debug purposes\n    if debug:\n        fig, axes = plt.subplots(1, 5, figsize=(20,10))\n        image0 = np.copy(image)\n        axes[0].imshow(image0)\n        axes[0].set_title('Original Image')\n        axes[0].axis('off')\n    else:\n        axes = False\n        \n    try:\n        image = voi_lut(image, dicom)\n    except:\n        pass\n\n    # Normalize [0,1] range\n    image = (image - image.min()) / (image.max() - image.min())\n\n    # Convert to uint8 image in range [0, 255]\n    image = (image * 255).astype(np.uint8)\n    \n    # Normalize to left/right orientation by flipping right/left oriented images\n    h0, w0 = image.shape\n    if image[:,int(-w0 * 0.10):].sum() > image[:,:int(w0 * 0.10)].sum():\n        image = np.flip(image, axis=1)\n    \n    if crop_image:\n        image = crop(image, size=size, debug=axes)\n    \n    # Resize\n    if size is not None:\n        # Pad black pixels to get correct image ratios\n        h, w = image.shape\n        if (h / w) > TARGET_HEIGHT_WIDTH_RATIO:\n            pad = int(h / TARGET_HEIGHT_WIDTH_RATIO - w)\n            image = np.pad(image, [[0,0], [0, pad]])\n            h, w = image.shape\n        else:\n            pad = int(0.50 * (w * TARGET_HEIGHT_WIDTH_RATIO - h))\n            image = np.pad(image, [[pad, pad], [0,0]])\n            h, w = image.shape\n        # Resize\n        image = cv2.resize(image, size, interpolation=cv2.INTER_AREA)\n        \n    # Apply CLAHE contrast enhancement\n    # https://docs.opencv.org/4.x/d5/daf/tutorial_py_histogram_equalization.html\n    #if apply_clahe:\n    #    image = CLAHE.apply(image)\n        \n    # Apply Histogram Equalization\n    # https://docs.opencv.org/4.x/d5/daf/tutorial_py_histogram_equalization.html\n    if apply_eq_hist:\n        image = cv2.equalizeHist(image)\n        \n    if debug:\n        axes[4].imshow(image)\n        axes[4].set_title('Processed Image')\n        axes[4].axis('off')\n        plt.show()\n\n    # Return Cancer Target\n    if ret_target:\n        #patient_id = int(file_path.split('/')[-2])\n        #image_id = int(file_path.split('/')[-1].split('.')[0])\n\n        target = 1\n        \n        return image, target\n    # Return image Only\n    else:\n        if debug:\n            return image0, image\n        else:\n            return image","metadata":{"papermill":{"duration":0.023071,"end_time":"2023-01-30T03:36:57.961787","exception":false,"start_time":"2023-01-30T03:36:57.938716","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-08T05:08:31.470517Z","iopub.execute_input":"2023-02-08T05:08:31.471170Z","iopub.status.idle":"2023-02-08T05:08:31.488070Z","shell.execute_reply.started":"2023-02-08T05:08:31.471124Z","shell.execute_reply":"2023-02-08T05:08:31.487171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example Preprocessing","metadata":{"papermill":{"duration":0.005944,"end_time":"2023-01-30T03:36:57.973880","exception":false,"start_time":"2023-01-30T03:36:57.967936","status":"completed"},"tags":[]}},{"cell_type":"code","source":"N = 4 if IS_INTERACTIVE else 10\nfor fp in tqdm(train['file_path'].head(N)):\n    process(\n            fp,\n            crop_image=True,\n            size=(TARGET_WIDTH, TARGET_HEIGHT),\n            debug=True,\n        )","metadata":{"papermill":{"duration":29.981235,"end_time":"2023-01-30T03:37:28.494958","exception":false,"start_time":"2023-01-30T03:36:58.513723","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-08T05:08:31.489176Z","iopub.execute_input":"2023-02-08T05:08:31.489517Z","iopub.status.idle":"2023-02-08T05:08:56.211610Z","shell.execute_reply.started":"2023-02-08T05:08:31.489488Z","shell.execute_reply":"2023-02-08T05:08:56.210362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make Pairs of Views as input to the model\nFILE_PATHS_PAIRS = []\nfor row_idx, row in tqdm(train.iterrows(), total=len(train)):\n        FILE_PATHS_PAIRS.append(row[['patient_id', 'image_id']].values)\n        \nFILE_PATHS_PAIRS = np.array(FILE_PATHS_PAIRS, dtype=object)\nprint(f'FILE_PATHS_PAIRS shape: {FILE_PATHS_PAIRS.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:56.212906Z","iopub.execute_input":"2023-02-08T05:08:56.213267Z","iopub.status.idle":"2023-02-08T05:08:56.922254Z","shell.execute_reply.started":"2023-02-08T05:08:56.213233Z","shell.execute_reply":"2023-02-08T05:08:56.921173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FILE_PATHS_PAIRS","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:56.923616Z","iopub.execute_input":"2023-02-08T05:08:56.923935Z","iopub.status.idle":"2023-02-08T05:08:56.931028Z","shell.execute_reply.started":"2023-02-08T05:08:56.923905Z","shell.execute_reply":"2023-02-08T05:08:56.929885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put every image in a seperate TFRecord file\nN_CHUNKS = 13\nCHUNKS = np.array_split(FILE_PATHS_PAIRS, N_CHUNKS)\n\nprint(f'N_CHUNKS: {N_CHUNKS}, CHUNK len: {len(CHUNKS[0])}, shape: {CHUNKS[0].shape}')","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:56.932712Z","iopub.execute_input":"2023-02-08T05:08:56.933174Z","iopub.status.idle":"2023-02-08T05:08:56.944535Z","shell.execute_reply.started":"2023-02-08T05:08:56.933123Z","shell.execute_reply":"2023-02-08T05:08:56.943335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Single sample processing\ndef process_chunk(args):\n    patient_id, image_id = args\n    # Define file path\n\n    fp = glob.glob(f'/kaggle/input/ddsm-mammography-positive-case/DICOM/DICOM/{patient_id}_{image_id}/*.dcm')[0]\n    # Get processed image and target\n    image, target = process(fp, size=(TARGET_WIDTH, TARGET_HEIGHT), ret_target=True, crop_image=True)\n\n    # Make grayscale channel\n    image = np.expand_dims(image, 2)\n    \n    # Encode PNG\n    if IMAGE_FORMAT == 'PNG':\n        image_serialized = tf.io.encode_png(image, compression=9).numpy()\n    # Encode JPEG\n    else:\n        image_serialized = tf.io.encode_jpeg(image, quality=IMAGE_QUALITY, optimize_size=True).numpy()\n    \n    return image_serialized, target, int(patient_id.split('_')[-1]), int(image_id)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:56.946200Z","iopub.execute_input":"2023-02-08T05:08:56.946773Z","iopub.status.idle":"2023-02-08T05:08:56.958011Z","shell.execute_reply.started":"2023-02-08T05:08:56.946728Z","shell.execute_reply":"2023-02-08T05:08:56.956835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_tf_records(chunks):\n    for chunk_idx, chunk in enumerate(tqdm(chunks)):\n        print(f'===== GENERATING TFRECORDS {chunk_idx} =====')\n        tfrecord_name = f'batch_{chunk_idx}.tfrecords'\n        \n        # Create the actual TFRecords\n        options = tf.io.TFRecordOptions(compression_type='GZIP', compression_level=9)\n        with tf.io.TFRecordWriter(tfrecord_name, options=options) as file_writer:\n            # Process Samples in Chunk in Parallell\n            jobs = [joblib.delayed(process_chunk)(args) for args in chunk]\n            chunk_processed = joblib.Parallel(\n                n_jobs=cpu_count(),\n                verbose=0,\n                backend='multiprocessing',\n                prefer='threads',\n            )(jobs)\n            \n            # Add Processed Samples to TFRecord\n            for image, target, patient_id, image_id in chunk_processed:\n                record_bytes = tf.train.Example(features=tf.train.Features(feature={\n                    # Image\n                    'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n\n                    # target\n                    'target': tf.train.Feature(int64_list=tf.train.Int64List(value=[target])),\n                    \n                    # patient_id\n                    'patient_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[patient_id])),\n                    \n                    # image_id\n                    'image_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_id])),\n                })).SerializeToString()\n                file_writer.write(record_bytes)\n            \n# Create TFRecords\nif IS_INTERACTIVE:\n    to_tf_records(CHUNKS[:1])\nelse:\n    to_tf_records(CHUNKS)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:08:56.959706Z","iopub.execute_input":"2023-02-08T05:08:56.960055Z","iopub.status.idle":"2023-02-08T05:10:52.011046Z","shell.execute_reply.started":"2023-02-08T05:08:56.960024Z","shell.execute_reply":"2023-02-08T05:10:52.008863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 16 if IS_INTERACTIVE else 32","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:10:52.013822Z","iopub.execute_input":"2023-02-08T05:10:52.014505Z","iopub.status.idle":"2023-02-08T05:10:52.023566Z","shell.execute_reply.started":"2023-02-08T05:10:52.014435Z","shell.execute_reply":"2023-02-08T05:10:52.022398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to decode the TFRecords\ndef decode_tfrecord(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64),\n        'patient_id': tf.io.FixedLenFeature([], tf.int64),\n        'image_id': tf.io.FixedLenFeature([], tf.int64),\n    })\n        \n    if IMAGE_FORMAT == 'PNG':\n        image = tf.io.decode_png(features['image'], channels=N_CHANNELS)\n    else:\n        image = tf.io.decode_jpeg(features['image'], channels=N_CHANNELS)\n        \n    image = tf.reshape(image, [TARGET_HEIGHT, TARGET_WIDTH, N_CHANNELS])\n\n    target = features['target']\n    patient_id = features['patient_id']\n    image_id = features['image_id']\n    \n    return image, target, patient_id, image_id","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:10:52.025670Z","iopub.execute_input":"2023-02-08T05:10:52.026054Z","iopub.status.idle":"2023-02-08T05:10:52.041553Z","shell.execute_reply.started":"2023-02-08T05:10:52.026019Z","shell.execute_reply":"2023-02-08T05:10:52.039677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample TFRecord Dataset\ndef get_train_dataset():\n    # Read all TFRecord file paths\n    FNAMES_TRAIN_TFRECORDS = tf.io.gfile.glob('./*.tfrecords')\n    # initialize TFRecord dataset\n    train_dataset = tf.data.TFRecordDataset(FNAMES_TRAIN_TFRECORDS, num_parallel_reads=1, compression_type='GZIP')\n    # Decode samples by mapping with decode function\n    train_dataset = train_dataset.map(decode_tfrecord)\n    # Batch samples\n    train_dataset = train_dataset.batch(N)\n    \n    return train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:10:52.043399Z","iopub.execute_input":"2023-02-08T05:10:52.043842Z","iopub.status.idle":"2023-02-08T05:10:52.062125Z","shell.execute_reply.started":"2023-02-08T05:10:52.043786Z","shell.execute_reply":"2023-02-08T05:10:52.061003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shows a batch of images\ndef show_batch(dataset, rows=N, cols=1):\n    images, targets, patient_ids, image_ids = next(iter(dataset))\n    images = np.moveaxis(images, 3, 1)\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(cols*6, rows*10))\n    for r in range(rows):\n        for c in range(cols):\n            img = images[r,c]\n            axes[r].imshow(img)\n            if c == 0:\n                target = targets[r]\n                patient_id = patient_ids[r]\n                image_id = image_ids[r]\n                axes[r].set_title(f'target: {target}, patient_id: {patient_id}, image_id: {image_id}', fontsize=12, pad=16)\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:10:52.064310Z","iopub.execute_input":"2023-02-08T05:10:52.065074Z","iopub.status.idle":"2023-02-08T05:10:52.077432Z","shell.execute_reply.started":"2023-02-08T05:10:52.065025Z","shell.execute_reply":"2023-02-08T05:10:52.076299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show Example Batch\ntrain_dataset = get_train_dataset()\nshow_batch(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:10:52.079344Z","iopub.execute_input":"2023-02-08T05:10:52.080148Z","iopub.status.idle":"2023-02-08T05:10:57.310223Z","shell.execute_reply.started":"2023-02-08T05:10:52.080086Z","shell.execute_reply":"2023-02-08T05:10:57.308687Z"},"trusted":true},"execution_count":null,"outputs":[]}]}