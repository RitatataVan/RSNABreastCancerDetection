{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#数据集链接\n#https://www.kaggle.com/competitions/rsna-breast-cancer-detection\n#https://www.kaggle.com/datasets/canming/rsna-tfrecords-768x1344-dataset2\n#https://www.kaggle.com/datasets/gmhost/rsna-extra-positive-tfrecord/versions/3\n#https://www.kaggle.com/datasets/markwijkhuizen/keras-cv-attention-models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install ConvNextV2 Models From Keras CV Attention Models Pip Package\n!pip install -qq /kaggle/input/keras-cv-attention-models/keras_cv_attention_models-1.3.9-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:09:30.418744Z","iopub.execute_input":"2023-02-15T03:09:30.419361Z","iopub.status.idle":"2023-02-15T03:09:39.692615Z","shell.execute_reply.started":"2023-02-15T03:09:30.419223Z","shell.execute_reply":"2023-02-15T03:09:39.690805Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%writefile convext_m0.py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom tqdm.notebook import tqdm\nfrom multiprocessing import cpu_count\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom keras_cv_attention_models import convnext\n\nimport os\nimport time\nimport pickle\nimport math\nimport random\nimport sys\nimport cv2\nimport gc\nimport datetime\n\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Python Version: {sys.version}')\n\n# Save Versions\n\nnow = datetime.datetime.now().strftime(\"%d-%b-%Y %H-%M-%S\")\nnp.save(now, np.array([now]))\n\n# Mixed Precision Policy\n\n# float32 or mixed_float16 (mixed precision: compute float16, variable float32)\n# TPU is fast enough and has enough memory to use float32\npolicy = tf.keras.mixed_precision.Policy('float32')\ntf.keras.mixed_precision.set_global_policy(policy)\n\nprint(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\nprint(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n\n# Matplotlib Config\n\n# MatplotLib Global Settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.labelsize'] = 18\nmpl.rcParams['axes.titlesize'] = 24\n\n# Config\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    IS_TPU = True\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    IS_TPU = False\n    STRATEGY = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nN_REPLICAS = STRATEGY.num_replicas_in_sync\nprint(f'N_REPLICAS: {N_REPLICAS}, IS_TPU: {IS_TPU}')\n\n#/kaggle/input/rsna-tfrecords-768x1344-dataset\n\n# For TPU's the dataset needs to be stored in Google Cloud\n# Retrieve the Google Cloud location of the dataset\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('rsna-tfrecords-768x1344-dataset2')\n\nSEED = 43\nDEBUG = False\nVERSION = 'convext_m0'\n# Image dimensions\nIMG_HEIGHT = 1344\nIMG_WIDTH = 768\nN_CHANNELS = 1\nINPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 1)\nN_SAMPLES_TFRECORDS = 548\n\n# Peak Learning Rate\nLR_MAX = 5e-6 * N_REPLICAS\nWD_RATIO = 0.008\n\nN_WARMUP_EPOCHS = 0\nN_EPOCHS = 10\n\n# Batch size\nBATCH_SIZE = 8 * N_REPLICAS\n\n# Is Interactive Flag and COrresponding Verbosity Method\nIS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\nVERBOSE = 1 if IS_INTERACTIVE else 2\n\n# Tensorflow AUTO flag\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(f'BATCH_SIZE: {BATCH_SIZE}')\n\n# Seed\n\n# Seed all random number generators\ndef seed_everything(seed=SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()\n\n# Train\n\n# Train DataFrame\ntrain = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n\n\n# Utility Functions\n\n# short Tensorflow randin integer function\ndef tf_rand_int(minval, maxval, dtype=tf.int64):\n    minval = tf.cast(minval, dtype)\n    maxval = tf.cast(maxval, dtype)\n    return tf.random.uniform(shape=(), minval=minval, maxval=maxval, dtype=dtype)\n\n# chance of 1 in k\ndef one_in(k):\n    return 0 == tf_rand_int(0, k)\n\n# Dataset\n\n# Function to benchmark the dataset\ndef benchmark_dataset(dataset, num_epochs=3, n_steps_per_epoch=10, bs=BATCH_SIZE):\n    start_time = time.perf_counter()\n    for epoch_num in range(num_epochs):\n        for idx, (inputs, labels) in enumerate(dataset.take(n_steps_per_epoch + 1)):\n            if idx == 0:\n                epoch_start = time.perf_counter()\n            elif idx == 1 and epoch_num == 0:\n                image = inputs['image']\n                print(f'image shape: {image.shape}, labels shape: {labels.shape}, image dtype: {image.dtype}, labels dtype: {labels.dtype}')\n            else:\n                pass\n        \n        epoch_t = time.perf_counter() - epoch_start\n        mean_step_t = round(epoch_t / n_steps_per_epoch * 1000, 1)\n        n_imgs_per_s = int(1 / (mean_step_t / 1000) * bs)\n        print(f'epoch {epoch_num} took: {round(epoch_t, 2)} sec, mean step duration: {mean_step_t}ms, images/s: {n_imgs_per_s}')\n\n# Plots a batch of images\ndef show_batch(dataset, n_rows=16, n_cols=4):\n    inputs, targets = next(iter(dataset))\n    images = inputs['image'].numpy().squeeze()\n    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols*4, n_rows*7))\n    for r in range(n_rows):\n        for c in range(n_cols):\n            idx = r * n_cols + c\n            # Image\n            img = images[idx]\n            axes[r, c].imshow(img)\n            # Target\n            target = targets[idx]\n            axes[r, c].set_title(f'target: {target}', fontsize=16, pad=5)\n        \n    plt.show()\n\n# Decodes the TFRecords\ndef decode_image(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64),\n        'patient_id': tf.io.FixedLenFeature([], tf.int64),\n    })\n    \n    # Decode PNG Image\n    image = tf.io.decode_jpeg(features['image'], channels=N_CHANNELS)\n    # Explicit reshape needed for TPU\n    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, N_CHANNELS])\n\n    target = features['target']\n    \n    return { 'image': image }, target\n\ndef augment_image(X, y):\n    image = X['image']\n    \n    # Random Brightness\n    image = tf.image.random_brightness(image, 0.10)\n    \n    # Random Contrast\n    image = tf.image.random_contrast(image, 0.90, 1.10)\n    \n    # Random JPEG Quality\n    image = tf.image.random_jpeg_quality(image, 75, 100)\n    \n    # Random crop image with maximum of 10%\n    ratio = tf.random.uniform([], 0.75, 1.00)\n    img_height_crop = tf.cast(ratio * IMG_HEIGHT, tf.int32)\n    img_width_crop = tf.cast(ratio * IMG_WIDTH, tf.int32)\n    # Random offset for crop\n    img_height_offset = tf_rand_int(0, IMG_HEIGHT - img_height_crop)\n    img_width_offset = 0\n    # Crop And Resize\n    image = tf.slice(image, [img_height_offset, img_width_offset, 0], [img_height_crop, img_width_crop, N_CHANNELS])\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.BILINEAR)\n    # Clip pixel values in range [0,255] to prevent underflow/overflow\n    image = tf.clip_by_value(image, 0, 255)\n    image = tf.cast(image, tf.uint8)\n    \n    return { 'image': image }, y\n\n# Undersample majority class (0/negative) by randomly dropping them\ndef undersample_majority(X, y):\n    # Filter 2/3 of negative samples to upsample positive samples by a factor 3\n    return y == 1 or tf.random.uniform([]) > 0.40\n\n\n# TFRecord file paths\nTFRECORDS_FILE_PATHS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH}/*.tfrecords'))\nprint(f'Found {len(TFRECORDS_FILE_PATHS)} TFRecords')\n\n# Train Test Split\nTFRECORDS_TRAIN, TFRECORDS_VAL = train_test_split(TFRECORDS_FILE_PATHS, train_size=0.80, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\n#TFRECORDS_TRAIN2, TFRECORDS_VAL2 = train_test_split(TFRECORDS_TRAIN, train_size=0.75, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN2)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL2)}')\n#TFRECORDS_TRAIN = [x  for x in TFRECORDS_FILE_PATHS if x not in TFRECORDS_VAL2]\n#TFRECORDS_VAL = TFRECORDS_VAL2\nGCS_DS_PATH_POS = KaggleDatasets().get_gcs_path('rsna-extra-positive-tfrecord')\nTFRECORDS_FILE_PATHS_POS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH_POS}/*.tfrecords'))\n\nTFRECORDS_TRAIN = TFRECORDS_TRAIN+TFRECORDS_FILE_PATHS_POS\nprint(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\ndef get_dataset(tfrecords, bs=BATCH_SIZE, val=False, debug=True):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    \n    # Initialize dataset with TFRecords\n    dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=AUTO, compression_type='GZIP')\n    \n    # Decode mapping\n    dataset = dataset.map(decode_image, num_parallel_calls=AUTO)\n\n    if not val:\n        dataset = dataset.filter(undersample_majority)\n        dataset = dataset.map(augment_image, num_parallel_calls=AUTO)\n        dataset = dataset.with_options(ignore_order)\n        if not debug:\n            dataset = dataset.shuffle(1024)\n        dataset = dataset.repeat()        \n\n    dataset = dataset.batch(bs, drop_remainder=not val)\n    dataset = dataset.prefetch(AUTO)\n    \n    return dataset\n\n# Get Train/Validation datasets\ntrain_dataset = get_dataset(TFRECORDS_TRAIN, val=False, debug=False)\nval_dataset = get_dataset(TFRECORDS_VAL, val=True, debug=False)\n\nTRAIN_STEPS_PER_EPOCH = len(TFRECORDS_TRAIN) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nVAL_STEPS_PER_EPOCH = len(TFRECORDS_VAL) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nprint(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}, VAL_STEPS_PER_EPOCH: {VAL_STEPS_PER_EPOCH}')\n\n# Sanity check, image and label statistics\nX_batch, y_batch = next(iter(get_dataset(TFRECORDS_TRAIN, val=False)))\nimage = X_batch['image'].numpy()\nprint(f'image shape: {image.shape}, y_batch shape: {y_batch.shape}')\nprint(f'image dtype: {image.dtype}, y_batch dtype: {y_batch.dtype}')\nprint(f'image min: {image.min():.2f}, max: {image.max():.2f}')\n\n# Benchmark Dataset\nbenchmark_dataset(get_dataset(TFRECORDS_TRAIN, val=False))\n\n# Show what we will be training on\nshow_batch(get_dataset(TFRECORDS_TRAIN, bs=64, val=False))\n\n# Tensorflow custom metric is just a conventional class object\nclass pF1(tf.keras.metrics.Metric):\n    # Initialize properties\n    def __init__(self, name='pF1', **kwargs):\n        super(pF1, self).__init__(name=name, **kwargs)\n        self.tc = self.add_weight(name='tc', initializer='zeros')\n        self.tp = self.add_weight(name='tp', initializer='zeros')\n        self.fp = self.add_weight(name='fp', initializer='zeros')\n\n    # Update state called on each batch with true and predicted labels\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.tc.assign_add(tf.cast(tf.reduce_sum(y_true), tf.float32))\n        self.tp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 1])), tf.float32))\n        self.fp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 0])), tf.float32))\n\n    # Result function is called to obtain result which is printed in progress bar\n    def result(self):\n        if self.tc == 0 or (self.tp + self.fp) == 0:\n            return 0.0\n        else:\n            precision = self.tp / (self.tp + self.fp)\n            recall = self.tp / (self.tc)\n            return 2 * (precision * recall) / (precision + recall)\n\n    # Reset state is called after each epoch to start fresh each epoch\n    def reset_state(self):\n        self.tc.assign(0)\n        self.tp.assign(0)\n        self.fp.assign(0)\n\ndef normalize(image):\n    # Repeat channels to create 3 channel images required by pretrained ConvNextV2 models\n    image = tf.repeat(image, repeats=3, axis=3)\n    # Cast to float 32\n    image = tf.cast(image, tf.float32)\n    # Normalize with respect to ImageNet mean/std\n    image = tf.keras.applications.imagenet_utils.preprocess_input(image, mode='torch')\n\n    return image\n\ndef get_model():\n    # Verify Mixed Policy Settings\n    print(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\n    print(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n    \n    with STRATEGY.scope():\n        # Set seed for deterministic weights initialization\n        seed_everything()\n        \n        # Inputs, note the names are equal to the dictionary keys in the dataset\n        image = tf.keras.layers.Input(INPUT_SHAPE, name='image', dtype=tf.uint8)\n        \n        # Normalize Input\n        image_norm = normalize(image)\n\n        # CNN Prediction in range [0,1]\n        x = convnext.ConvNeXtV2Tiny(\n            input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n            pretrained='imagenet21k-ft1k',\n            num_classes=0,\n        )(image_norm)\n        \n        # Average Pooling BxHxWxC -> BxC\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        # Dropout to prevent Overfitting\n        x = tf.keras.layers.Dropout(0.30)(x)\n        # Output value between [0, 1] using Sigmoid function\n        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n        # We will use the famous AdamW optimizer for fast learning with weight decay\n        optimizer = tfa.optimizers.AdamW(learning_rate=LR_MAX, weight_decay=LR_MAX*WD_RATIO, epsilon=1e-6)\n\n        # Loss\n        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n        \n        # Metrics\n        metrics = [\n            pF1(),\n            tfa.metrics.F1Score(num_classes=1, threshold=0.50),\n            tf.keras.metrics.Precision(),\n            tf.keras.metrics.Recall(),\n            tf.keras.metrics.AUC(),\n            tf.keras.metrics.BinaryAccuracy(),\n        ]\n\n        model = tf.keras.models.Model(inputs=image, outputs=outputs)\n        \n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n        return model\n\n# Pretrained File Path: '/kaggle/input/sartorius-training-dataset/model.h5'\ntf.keras.backend.clear_session()\n# enable XLA optmizations\ntf.config.optimizer.set_jit(True)\n\nmodel = get_model()\n\n# Weight Initilization\n\n# Validation metric on initialized model\n_ = model.evaluate(\n        get_dataset(TFRECORDS_VAL, val=True),\n        verbose=VERBOSE,\n        steps=VAL_STEPS_PER_EPOCH,\n    )\n\n# Learning rate scheduler with logaritmic warmup and cosine decay\ndef lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n    \n    if current_step < num_warmup_steps:\n        return lr_max * 0.10 ** (num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n\n# Plot the learning rate scheduler\ndef plot_lr_schedule(lr_schedule, epochs):\n    fig = plt.figure(figsize=(20, 10))\n    plt.plot([None] + lr_schedule + [None])\n    # X Labels\n    x = np.arange(1, epochs + 1)\n    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n    plt.xlim([1, epochs])\n    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n    \n    # Increase y-limit for better readability\n    plt.ylim([0, max(lr_schedule) * 1.1])\n    \n    # Title\n    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n    \n    # Plot Learning Rates\n    for x, val in enumerate(lr_schedule):\n        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n            if x < len(lr_schedule) - 1:\n                if lr_schedule[x - 1] < val:\n                    ha = 'right'\n                else:\n                    ha = 'left'\n            elif x == 0:\n                ha = 'right'\n            else:\n                ha = 'left'\n            plt.plot(x + 1, val, 'o', color='black');\n            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n    \n    plt.xlabel('Epoch', size=16, labelpad=5)\n    plt.ylabel('Learning Rate', size=16, labelpad=5)\n    plt.grid()\n    plt.show()\n\n# Learning rate for encoder\nLR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\nplot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n\n# Learning Rate Callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n\n# Weight Decay Callback\n\n# Tensorflow Learning Rate Scheduler does not update weight decay, need to do it manually in a custom callback\nclass WeightDecayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, wd_ratio=WD_RATIO):\n        self.step_counter = 0\n        self.wd_ratio = wd_ratio\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')\n\n# Train model on TPU!\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=f'best_{VERSION}.h5',\n    save_weights_only=True,\n    monitor='val_pF1',\n    mode='max',\n    save_best_only=True)\nhistory = model.fit(\n        train_dataset,\n        steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n        validation_data = val_dataset,\n        epochs = N_EPOCHS,\n        verbose = VERBOSE,\n        callbacks = [\n            model_checkpoint_callback,\n            lr_callback,\n            WeightDecayCallback(),\n        ],\n        class_weight = {\n            0: 1.0,\n            1: 3.0,\n        },\n    )\n\n# Save model weights for inference\nmodel.save_weights(f'model_{VERSION}.h5')\n\n# F1 By Threshold\n\n# Get true labels and predictions for validation set\ny_true_val = []\ny_pred_val = []\nfor X_batch, y_batch in tqdm(get_dataset(TFRECORDS_VAL, val=True), total=VAL_STEPS_PER_EPOCH):\n    y_true_val += y_batch.numpy().tolist()\n    y_pred_val += model.predict_on_batch(X_batch).squeeze().tolist()\n\n# source: https://www.kaggle.com/code/sohier/probabilistic-f-score\n# Competition Leaderboard Metric\ndef pfbeta(labels, predictions, beta=1):\n    y_true_count = 0\n    ctp = 0\n    cfp = 0\n\n    for idx in range(len(labels)):\n        prediction = min(max(predictions[idx], 0), 1)\n        if (labels[idx]):\n            y_true_count += 1\n            ctp += prediction\n        else:\n            cfp += prediction\n\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp)\n    c_recall = ctp / y_true_count\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n        return result\n    else:\n        return 0\n\n# Plot pF1 by threshold plot to find best threshold\npf1_by_threshold = []\nthresholds = np.arange(0, 1.01, 0.01)\nfor t in tqdm(thresholds):\n    # Compute pF1 for each threshold\n    pf1_by_threshold.append(\n        pfbeta(y_true_val, y_pred_val > t)\n    )\n    \nplt.figure(figsize=(15,8))\nplt.title('F1 By Threshold', size=24)\nplt.plot(pf1_by_threshold, label='F1 Score')\n\n# Best threshold and pF1 score\narg_max = np.argmax(pf1_by_threshold)\nval_max = np.max(pf1_by_threshold)\nthreshold_best = thresholds[arg_max]\nplt.scatter(arg_max, val_max, color='red', label=f'Best Threshold {threshold_best:.2f}, pF1 Score: {val_max:.2f}')\n\n# Plot pF1 by Threshold\nplt.xticks(np.arange(0, 110, 10), [f'{t:.2f}' for t in np.arange(0, 1.1, 0.1)])\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.xlim(0, 100)\nplt.ylim(0, 1)\nplt.xlabel('Threshold')\nplt.ylabel('pF1 Score')\nplt.legend(fontsize=12)\nplt.grid()\nplt.show()\nprint(f'Best Threshold {threshold_best:.2f}.')\n\nprint(f'pF1 Score: {val_max:.2f}.')\n\n# Training History","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:09:39.695854Z","iopub.execute_input":"2023-02-15T03:09:39.696526Z","iopub.status.idle":"2023-02-15T03:09:39.713717Z","shell.execute_reply.started":"2023-02-15T03:09:39.696487Z","shell.execute_reply":"2023-02-15T03:09:39.712487Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Writing convext_m0.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python convext_m0.py","metadata":{"execution":{"iopub.status.busy":"2023-02-15T03:09:39.715488Z","iopub.execute_input":"2023-02-15T03:09:39.716420Z","iopub.status.idle":"2023-02-15T04:21:14.322013Z","shell.execute_reply.started":"2023-02-15T03:09:39.716376Z","shell.execute_reply":"2023-02-15T04:21:14.321123Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2023-02-15 03:09:41.082815: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 03:09:41.082925: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTensorflow Version: 2.4.1\nPython Version: 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \n[GCC 9.3.0]\nCompute dtype: float32\nVariable dtype: float32\nRunning on TPU  grpc://10.0.0.2:8470\n2023-02-15 03:09:49.474260: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 03:09:49.478555: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 03:09:49.478628: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2023-02-15 03:09:49.478658: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (375a1fcc8acc): /proc/driver/nvidia/version does not exist\n2023-02-15 03:09:49.482107: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-15 03:09:49.484208: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 03:09:49.489736: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 03:09:49.517950: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 03:09:49.518155: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30053}\n2023-02-15 03:09:49.535998: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 03:09:49.536150: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30053}\n2023-02-15 03:09:49.538859: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30053\nN_REPLICAS: 8, IS_TPU: True\nBATCH_SIZE: 64\n2023-02-15 03:09:57.676784: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\nFound 100 TFRecords\n2023-02-15 03:10:01.671957: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n# TFRECORDS_TRAIN: 83, # TFRECORDS_VAL: 20\nTRAIN_STEPS_PER_EPOCH: 710, VAL_STEPS_PER_EPOCH: 171\nimage shape: (64, 1344, 768, 1), y_batch shape: (64,)\nimage dtype: uint8, y_batch dtype: <dtype: 'int64'>\nimage min: 0.00, max: 255.00\nimage shape: (64, 1344, 768, 1), labels shape: (64,), image dtype: <dtype: 'uint8'>, labels dtype: <dtype: 'int64'>\nepoch 0 took: 1.15 sec, mean step duration: 115.2ms, images/s: 555\n2023-02-15 03:10:16.368594: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 1\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676430616.365078848\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 1\",\"grpc_status\":3}\nepoch 1 took: 1.18 sec, mean step duration: 118.4ms, images/s: 540\nepoch 2 took: 1.1 sec, mean step duration: 110.4ms, images/s: 579\nCompute dtype: float32\nVariable dtype: float32\nDownloading data from https://github.com/leondgarse/keras_cv_attention_models/releases/download/convnext/convnext_v2_tiny_384_imagenet21k-ft1k.h5\n115146752/115145456 [==============================] - 6s 0us/step\n>>>> Load pretrained from: /root/.keras/models/convnext_v2_tiny_384_imagenet21k-ft1k.h5\n171/171 [==============================] - 59s 207ms/step - loss: 0.8047 - pF1: 0.0401 - f1_score: 0.0389 - precision: 0.0201 - recall: 0.6223 - auc: 0.4718 - binary_accuracy: 0.3402\nEpoch 1/10\nlearning rate: 4.00e-05, weight decay: 3.20e-07\n710/710 [==============================] - ETA: 0s - loss: 0.3695 - pF1: 0.3356 - f1_score: 0.5767 - precision: 0.7921 - recall: 0.4534 - auc: 0.7615 - binary_accuracy: 0.95682023-02-15 03:19:39.240554: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 4\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676431179.240063544\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 4\",\"grpc_status\":3}\n710/710 [==============================] - 479s 577ms/step - loss: 0.3695 - pF1: 0.3356 - f1_score: 0.5767 - precision: 0.7921 - recall: 0.4534 - auc: 0.7615 - binary_accuracy: 0.9568 - val_loss: 0.0965 - val_pF1: 0.0445 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7350 - val_binary_accuracy: 0.9792\nEpoch 2/10\nlearning rate: 3.90e-05, weight decay: 3.12e-07\n710/710 [==============================] - 394s 554ms/step - loss: 0.2784 - pF1: 0.5722 - f1_score: 0.7708 - precision: 0.8486 - recall: 0.7060 - auc: 0.9168 - binary_accuracy: 0.9608 - val_loss: 0.1561 - val_pF1: 0.0873 - val_f1_score: 0.2048 - val_precision: 0.3269 - val_recall: 0.1491 - val_auc: 0.7859 - val_binary_accuracy: 0.9759\nEpoch 3/10\nlearning rate: 3.62e-05, weight decay: 2.89e-07\n710/710 [==============================] - 393s 553ms/step - loss: 0.2311 - pF1: 0.5073 - f1_score: 0.7003 - precision: 0.7543 - recall: 0.6535 - auc: 0.9218 - binary_accuracy: 0.9644 - val_loss: 0.1093 - val_pF1: 0.1417 - val_f1_score: 0.2539 - val_precision: 0.3101 - val_recall: 0.2149 - val_auc: 0.7817 - val_binary_accuracy: 0.9737\nEpoch 4/10\nlearning rate: 3.18e-05, weight decay: 2.54e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.1856 - pF1: 0.6960 - f1_score: 0.8344 - precision: 0.8486 - recall: 0.8207 - auc: 0.9667 - binary_accuracy: 0.9695 - val_loss: 0.0827 - val_pF1: 0.1926 - val_f1_score: 0.2727 - val_precision: 0.5250 - val_recall: 0.1842 - val_auc: 0.8172 - val_binary_accuracy: 0.9795\nEpoch 5/10\nlearning rate: 2.62e-05, weight decay: 2.09e-07\n710/710 [==============================] - 394s 555ms/step - loss: 0.1390 - pF1: 0.7598 - f1_score: 0.8693 - precision: 0.8716 - recall: 0.8671 - auc: 0.9819 - binary_accuracy: 0.9757 - val_loss: 0.1196 - val_pF1: 0.2206 - val_f1_score: 0.3115 - val_precision: 0.2611 - val_recall: 0.3860 - val_auc: 0.8088 - val_binary_accuracy: 0.9644\nEpoch 6/10\nlearning rate: 2.00e-05, weight decay: 1.60e-07\n710/710 [==============================] - 395s 557ms/step - loss: 0.1042 - pF1: 0.7432 - f1_score: 0.8591 - precision: 0.8539 - recall: 0.8644 - auc: 0.9842 - binary_accuracy: 0.9818 - val_loss: 0.1056 - val_pF1: 0.2394 - val_f1_score: 0.3100 - val_precision: 0.3605 - val_recall: 0.2719 - val_auc: 0.7846 - val_binary_accuracy: 0.9748\nEpoch 7/10\nlearning rate: 1.38e-05, weight decay: 1.11e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.0733 - pF1: 0.8639 - f1_score: 0.9289 - precision: 0.9256 - recall: 0.9322 - auc: 0.9947 - binary_accuracy: 0.9867 - val_loss: 0.1237 - val_pF1: 0.2348 - val_f1_score: 0.2817 - val_precision: 0.3030 - val_recall: 0.2632 - val_auc: 0.7315 - val_binary_accuracy: 0.9720\nEpoch 8/10\nlearning rate: 8.24e-06, weight decay: 6.60e-08\n710/710 [==============================] - 395s 556ms/step - loss: 0.0570 - pF1: 0.8843 - f1_score: 0.9401 - precision: 0.9334 - recall: 0.9469 - auc: 0.9957 - binary_accuracy: 0.9899 - val_loss: 0.1210 - val_pF1: 0.2208 - val_f1_score: 0.2586 - val_precision: 0.3245 - val_recall: 0.2149 - val_auc: 0.7222 - val_binary_accuracy: 0.9743\nEpoch 9/10\nlearning rate: 3.82e-06, weight decay: 3.06e-08\n710/710 [==============================] - 393s 554ms/step - loss: 0.0455 - pF1: 0.8909 - f1_score: 0.9431 - precision: 0.9350 - recall: 0.9514 - auc: 0.9965 - binary_accuracy: 0.9915 - val_loss: 0.1175 - val_pF1: 0.2419 - val_f1_score: 0.2659 - val_precision: 0.3898 - val_recall: 0.2018 - val_auc: 0.7253 - val_binary_accuracy: 0.9768\nEpoch 10/10\nlearning rate: 9.79e-07, weight decay: 7.83e-09\n710/710 [==============================] - 393s 554ms/step - loss: 0.0414 - pF1: 0.9183 - f1_score: 0.9605 - precision: 0.9574 - recall: 0.9635 - auc: 0.9980 - binary_accuracy: 0.9926 - val_loss: 0.1208 - val_pF1: 0.2492 - val_f1_score: 0.2755 - val_precision: 0.3704 - val_recall: 0.2193 - val_auc: 0.7321 - val_binary_accuracy: 0.9760\n  0%|          | 0/171 [00:00<?, ?it/s]\n2023-02-15 04:21:04.160519: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 189718, Output num: 1\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676434864.160264973\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 189718, Output num: 1\",\"grpc_status\":3}\n  0%|          | 0/101 [00:00<?, ?it/s]\nconvext_m0.py:511: RuntimeWarning: invalid value encountered in long_scalars\n  c_precision = ctp / (ctp + cfp)\nBest Threshold 0.29.\npF1 Score: 0.29.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile convext_m1.py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom tqdm.notebook import tqdm\nfrom multiprocessing import cpu_count\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom keras_cv_attention_models import convnext\n\nimport os\nimport time\nimport pickle\nimport math\nimport random\nimport sys\nimport cv2\nimport gc\nimport datetime\n\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Python Version: {sys.version}')\n\n# Save Versions\n\nnow = datetime.datetime.now().strftime(\"%d-%b-%Y %H-%M-%S\")\nnp.save(now, np.array([now]))\n\n# Mixed Precision Policy\n\n# float32 or mixed_float16 (mixed precision: compute float16, variable float32)\n# TPU is fast enough and has enough memory to use float32\npolicy = tf.keras.mixed_precision.Policy('float32')\ntf.keras.mixed_precision.set_global_policy(policy)\n\nprint(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\nprint(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n\n# Matplotlib Config\n\n# MatplotLib Global Settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.labelsize'] = 18\nmpl.rcParams['axes.titlesize'] = 24\n\n# Config\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    IS_TPU = True\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    IS_TPU = False\n    STRATEGY = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nN_REPLICAS = STRATEGY.num_replicas_in_sync\nprint(f'N_REPLICAS: {N_REPLICAS}, IS_TPU: {IS_TPU}')\n\n#/kaggle/input/rsna-tfrecords-768x1344-dataset\n\n# For TPU's the dataset needs to be stored in Google Cloud\n# Retrieve the Google Cloud location of the dataset\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('rsna-tfrecords-768x1344-dataset2')\n\nSEED = 43\nDEBUG = False\nVERSION = 'convext_m1'\n# Image dimensions\nIMG_HEIGHT = 1344\nIMG_WIDTH = 768\nN_CHANNELS = 1\nINPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 1)\nN_SAMPLES_TFRECORDS = 548\n\n# Peak Learning Rate\nLR_MAX = 5e-6 * N_REPLICAS\nWD_RATIO = 0.008\n\nN_WARMUP_EPOCHS = 0\nN_EPOCHS = 10\n\n# Batch size\nBATCH_SIZE = 8 * N_REPLICAS\n\n# Is Interactive Flag and COrresponding Verbosity Method\nIS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\nVERBOSE = 1 if IS_INTERACTIVE else 2\n\n# Tensorflow AUTO flag\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(f'BATCH_SIZE: {BATCH_SIZE}')\n\n# Seed\n\n# Seed all random number generators\ndef seed_everything(seed=SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()\n\n# Train\n\n# Train DataFrame\ntrain = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n\n\n# Utility Functions\n\n# short Tensorflow randin integer function\ndef tf_rand_int(minval, maxval, dtype=tf.int64):\n    minval = tf.cast(minval, dtype)\n    maxval = tf.cast(maxval, dtype)\n    return tf.random.uniform(shape=(), minval=minval, maxval=maxval, dtype=dtype)\n\n# chance of 1 in k\ndef one_in(k):\n    return 0 == tf_rand_int(0, k)\n\n# Dataset\n\n# Function to benchmark the dataset\ndef benchmark_dataset(dataset, num_epochs=3, n_steps_per_epoch=10, bs=BATCH_SIZE):\n    start_time = time.perf_counter()\n    for epoch_num in range(num_epochs):\n        for idx, (inputs, labels) in enumerate(dataset.take(n_steps_per_epoch + 1)):\n            if idx == 0:\n                epoch_start = time.perf_counter()\n            elif idx == 1 and epoch_num == 0:\n                image = inputs['image']\n                print(f'image shape: {image.shape}, labels shape: {labels.shape}, image dtype: {image.dtype}, labels dtype: {labels.dtype}')\n            else:\n                pass\n        \n        epoch_t = time.perf_counter() - epoch_start\n        mean_step_t = round(epoch_t / n_steps_per_epoch * 1000, 1)\n        n_imgs_per_s = int(1 / (mean_step_t / 1000) * bs)\n        print(f'epoch {epoch_num} took: {round(epoch_t, 2)} sec, mean step duration: {mean_step_t}ms, images/s: {n_imgs_per_s}')\n\n# Plots a batch of images\ndef show_batch(dataset, n_rows=16, n_cols=4):\n    inputs, targets = next(iter(dataset))\n    images = inputs['image'].numpy().squeeze()\n    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols*4, n_rows*7))\n    for r in range(n_rows):\n        for c in range(n_cols):\n            idx = r * n_cols + c\n            # Image\n            img = images[idx]\n            axes[r, c].imshow(img)\n            # Target\n            target = targets[idx]\n            axes[r, c].set_title(f'target: {target}', fontsize=16, pad=5)\n        \n    plt.show()\n\n# Decodes the TFRecords\ndef decode_image(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64),\n        'patient_id': tf.io.FixedLenFeature([], tf.int64),\n    })\n    \n    # Decode PNG Image\n    image = tf.io.decode_jpeg(features['image'], channels=N_CHANNELS)\n    # Explicit reshape needed for TPU\n    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, N_CHANNELS])\n\n    target = features['target']\n    \n    return { 'image': image }, target\n\ndef augment_image(X, y):\n    image = X['image']\n    \n    # Random Brightness\n    image = tf.image.random_brightness(image, 0.10)\n    \n    # Random Contrast\n    image = tf.image.random_contrast(image, 0.90, 1.10)\n    \n    # Random JPEG Quality\n    image = tf.image.random_jpeg_quality(image, 75, 100)\n    \n    # Random crop image with maximum of 10%\n    ratio = tf.random.uniform([], 0.75, 1.00)\n    img_height_crop = tf.cast(ratio * IMG_HEIGHT, tf.int32)\n    img_width_crop = tf.cast(ratio * IMG_WIDTH, tf.int32)\n    # Random offset for crop\n    img_height_offset = tf_rand_int(0, IMG_HEIGHT - img_height_crop)\n    img_width_offset = 0\n    # Crop And Resize\n    image = tf.slice(image, [img_height_offset, img_width_offset, 0], [img_height_crop, img_width_crop, N_CHANNELS])\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.BILINEAR)\n    # Clip pixel values in range [0,255] to prevent underflow/overflow\n    image = tf.clip_by_value(image, 0, 255)\n    image = tf.cast(image, tf.uint8)\n    \n    return { 'image': image }, y\n\n# Undersample majority class (0/negative) by randomly dropping them\ndef undersample_majority(X, y):\n    # Filter 2/3 of negative samples to upsample positive samples by a factor 3\n    return y == 1 or tf.random.uniform([]) > 0.40\n\n# TFRecord file paths\nTFRECORDS_FILE_PATHS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH}/*.tfrecords'))\nprint(f'Found {len(TFRECORDS_FILE_PATHS)} TFRecords')\n\n# Train Test Split\nTFRECORDS_TRAIN, TFRECORDS_VAL = train_test_split(TFRECORDS_FILE_PATHS, train_size=0.80, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\nTFRECORDS_TRAIN2, TFRECORDS_VAL2 = train_test_split(TFRECORDS_TRAIN, train_size=0.75, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN2)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL2)}')\nTFRECORDS_TRAIN = [x  for x in TFRECORDS_FILE_PATHS if x not in TFRECORDS_VAL2]\nTFRECORDS_VAL = TFRECORDS_VAL2\nGCS_DS_PATH_POS = KaggleDatasets().get_gcs_path('rsna-extra-positive-tfrecord')\nTFRECORDS_FILE_PATHS_POS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH_POS}/*.tfrecords'))\n\nTFRECORDS_TRAIN = TFRECORDS_TRAIN+TFRECORDS_FILE_PATHS_POS\nprint(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\ndef get_dataset(tfrecords, bs=BATCH_SIZE, val=False, debug=True):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    \n    # Initialize dataset with TFRecords\n    dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=AUTO, compression_type='GZIP')\n    \n    # Decode mapping\n    dataset = dataset.map(decode_image, num_parallel_calls=AUTO)\n\n    if not val:\n        dataset = dataset.filter(undersample_majority)\n        dataset = dataset.map(augment_image, num_parallel_calls=AUTO)\n        dataset = dataset.with_options(ignore_order)\n        if not debug:\n            dataset = dataset.shuffle(1024)\n        dataset = dataset.repeat()        \n\n    dataset = dataset.batch(bs, drop_remainder=not val)\n    dataset = dataset.prefetch(AUTO)\n    \n    return dataset\n\n# Get Train/Validation datasets\ntrain_dataset = get_dataset(TFRECORDS_TRAIN, val=False, debug=False)\nval_dataset = get_dataset(TFRECORDS_VAL, val=True, debug=False)\n\nTRAIN_STEPS_PER_EPOCH = len(TFRECORDS_TRAIN) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nVAL_STEPS_PER_EPOCH = len(TFRECORDS_VAL) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nprint(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}, VAL_STEPS_PER_EPOCH: {VAL_STEPS_PER_EPOCH}')\n\n# Sanity check, image and label statistics\nX_batch, y_batch = next(iter(get_dataset(TFRECORDS_TRAIN, val=False)))\nimage = X_batch['image'].numpy()\nprint(f'image shape: {image.shape}, y_batch shape: {y_batch.shape}')\nprint(f'image dtype: {image.dtype}, y_batch dtype: {y_batch.dtype}')\nprint(f'image min: {image.min():.2f}, max: {image.max():.2f}')\n\n# Benchmark Dataset\nbenchmark_dataset(get_dataset(TFRECORDS_TRAIN, val=False))\n\n# Show what we will be training on\nshow_batch(get_dataset(TFRECORDS_TRAIN, bs=64, val=False))\n\n# Tensorflow custom metric is just a conventional class object\nclass pF1(tf.keras.metrics.Metric):\n    # Initialize properties\n    def __init__(self, name='pF1', **kwargs):\n        super(pF1, self).__init__(name=name, **kwargs)\n        self.tc = self.add_weight(name='tc', initializer='zeros')\n        self.tp = self.add_weight(name='tp', initializer='zeros')\n        self.fp = self.add_weight(name='fp', initializer='zeros')\n\n    # Update state called on each batch with true and predicted labels\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.tc.assign_add(tf.cast(tf.reduce_sum(y_true), tf.float32))\n        self.tp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 1])), tf.float32))\n        self.fp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 0])), tf.float32))\n\n    # Result function is called to obtain result which is printed in progress bar\n    def result(self):\n        if self.tc == 0 or (self.tp + self.fp) == 0:\n            return 0.0\n        else:\n            precision = self.tp / (self.tp + self.fp)\n            recall = self.tp / (self.tc)\n            return 2 * (precision * recall) / (precision + recall)\n\n    # Reset state is called after each epoch to start fresh each epoch\n    def reset_state(self):\n        self.tc.assign(0)\n        self.tp.assign(0)\n        self.fp.assign(0)\n\ndef normalize(image):\n    # Repeat channels to create 3 channel images required by pretrained ConvNextV2 models\n    image = tf.repeat(image, repeats=3, axis=3)\n    # Cast to float 32\n    image = tf.cast(image, tf.float32)\n    # Normalize with respect to ImageNet mean/std\n    image = tf.keras.applications.imagenet_utils.preprocess_input(image, mode='torch')\n\n    return image\n\ndef get_model():\n    # Verify Mixed Policy Settings\n    print(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\n    print(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n    \n    with STRATEGY.scope():\n        # Set seed for deterministic weights initialization\n        seed_everything()\n        \n        # Inputs, note the names are equal to the dictionary keys in the dataset\n        image = tf.keras.layers.Input(INPUT_SHAPE, name='image', dtype=tf.uint8)\n        \n        # Normalize Input\n        image_norm = normalize(image)\n\n        # CNN Prediction in range [0,1]\n        x = convnext.ConvNeXtV2Tiny(\n            input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n            pretrained='imagenet21k-ft1k',\n            num_classes=0,\n        )(image_norm)\n        \n        # Average Pooling BxHxWxC -> BxC\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        # Dropout to prevent Overfitting\n        x = tf.keras.layers.Dropout(0.30)(x)\n        # Output value between [0, 1] using Sigmoid function\n        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n        # We will use the famous AdamW optimizer for fast learning with weight decay\n        optimizer = tfa.optimizers.AdamW(learning_rate=LR_MAX, weight_decay=LR_MAX*WD_RATIO, epsilon=1e-6)\n\n        # Loss\n        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n        \n        # Metrics\n        metrics = [\n            pF1(),\n            tfa.metrics.F1Score(num_classes=1, threshold=0.50),\n            tf.keras.metrics.Precision(),\n            tf.keras.metrics.Recall(),\n            tf.keras.metrics.AUC(),\n            tf.keras.metrics.BinaryAccuracy(),\n        ]\n\n        model = tf.keras.models.Model(inputs=image, outputs=outputs)\n        \n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n        return model\n\n# Pretrained File Path: '/kaggle/input/sartorius-training-dataset/model.h5'\ntf.keras.backend.clear_session()\n# enable XLA optmizations\ntf.config.optimizer.set_jit(True)\n\nmodel = get_model()\n\n# Weight Initilization\n\n# Validation metric on initialized model\n_ = model.evaluate(\n        get_dataset(TFRECORDS_VAL, val=True),\n        verbose=VERBOSE,\n        steps=VAL_STEPS_PER_EPOCH,\n    )\n\n# Learning rate scheduler with logaritmic warmup and cosine decay\ndef lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n    \n    if current_step < num_warmup_steps:\n        return lr_max * 0.10 ** (num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n\n# Plot the learning rate scheduler\ndef plot_lr_schedule(lr_schedule, epochs):\n    fig = plt.figure(figsize=(20, 10))\n    plt.plot([None] + lr_schedule + [None])\n    # X Labels\n    x = np.arange(1, epochs + 1)\n    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n    plt.xlim([1, epochs])\n    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n    \n    # Increase y-limit for better readability\n    plt.ylim([0, max(lr_schedule) * 1.1])\n    \n    # Title\n    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n    \n    # Plot Learning Rates\n    for x, val in enumerate(lr_schedule):\n        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n            if x < len(lr_schedule) - 1:\n                if lr_schedule[x - 1] < val:\n                    ha = 'right'\n                else:\n                    ha = 'left'\n            elif x == 0:\n                ha = 'right'\n            else:\n                ha = 'left'\n            plt.plot(x + 1, val, 'o', color='black');\n            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n    \n    plt.xlabel('Epoch', size=16, labelpad=5)\n    plt.ylabel('Learning Rate', size=16, labelpad=5)\n    plt.grid()\n    plt.show()\n\n# Learning rate for encoder\nLR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\nplot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n\n# Learning Rate Callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n\n# Weight Decay Callback\n\n# Tensorflow Learning Rate Scheduler does not update weight decay, need to do it manually in a custom callback\nclass WeightDecayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, wd_ratio=WD_RATIO):\n        self.step_counter = 0\n        self.wd_ratio = wd_ratio\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')\n\n# Train model on TPU!\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=f'best_{VERSION}.h5',\n    save_weights_only=True,\n    monitor='val_pF1',\n    mode='max',\n    save_best_only=True)\nhistory = model.fit(\n        train_dataset,\n        steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n        validation_data = val_dataset,\n        epochs = N_EPOCHS,\n        verbose = VERBOSE,\n        callbacks = [\n            model_checkpoint_callback,\n            lr_callback,\n            WeightDecayCallback(),\n        ],\n        class_weight = {\n            0: 1.0,\n            1: 3.0,\n        },\n    )\n\n# Save model weights for inference\nmodel.save_weights(f'model_{VERSION}.h5')\n\n# F1 By Threshold\n\n# Get true labels and predictions for validation set\ny_true_val = []\ny_pred_val = []\nfor X_batch, y_batch in tqdm(get_dataset(TFRECORDS_VAL, val=True), total=VAL_STEPS_PER_EPOCH):\n    y_true_val += y_batch.numpy().tolist()\n    y_pred_val += model.predict_on_batch(X_batch).squeeze().tolist()\n\n# source: https://www.kaggle.com/code/sohier/probabilistic-f-score\n# Competition Leaderboard Metric\ndef pfbeta(labels, predictions, beta=1):\n    y_true_count = 0\n    ctp = 0\n    cfp = 0\n\n    for idx in range(len(labels)):\n        prediction = min(max(predictions[idx], 0), 1)\n        if (labels[idx]):\n            y_true_count += 1\n            ctp += prediction\n        else:\n            cfp += prediction\n\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp)\n    c_recall = ctp / y_true_count\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n        return result\n    else:\n        return 0\n\n# Plot pF1 by threshold plot to find best threshold\npf1_by_threshold = []\nthresholds = np.arange(0, 1.01, 0.01)\nfor t in tqdm(thresholds):\n    # Compute pF1 for each threshold\n    pf1_by_threshold.append(\n        pfbeta(y_true_val, y_pred_val > t)\n    )\n    \nplt.figure(figsize=(15,8))\nplt.title('F1 By Threshold', size=24)\nplt.plot(pf1_by_threshold, label='F1 Score')\n\n# Best threshold and pF1 score\narg_max = np.argmax(pf1_by_threshold)\nval_max = np.max(pf1_by_threshold)\nthreshold_best = thresholds[arg_max]\nplt.scatter(arg_max, val_max, color='red', label=f'Best Threshold {threshold_best:.2f}, pF1 Score: {val_max:.2f}')\n\n# Plot pF1 by Threshold\nplt.xticks(np.arange(0, 110, 10), [f'{t:.2f}' for t in np.arange(0, 1.1, 0.1)])\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.xlim(0, 100)\nplt.ylim(0, 1)\nplt.xlabel('Threshold')\nplt.ylabel('pF1 Score')\nplt.legend(fontsize=12)\nplt.grid()\nplt.show()\nprint(f'Best Threshold {threshold_best:.2f}.')\n\nprint(f'pF1 Score: {val_max:.2f}.')\n\n# Training History","metadata":{"execution":{"iopub.status.busy":"2023-02-15T04:21:14.324463Z","iopub.execute_input":"2023-02-15T04:21:14.324706Z","iopub.status.idle":"2023-02-15T04:21:14.336902Z","shell.execute_reply.started":"2023-02-15T04:21:14.324677Z","shell.execute_reply":"2023-02-15T04:21:14.336391Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Writing convext_m1.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python convext_m1.py","metadata":{"execution":{"iopub.status.busy":"2023-02-15T04:21:14.337847Z","iopub.execute_input":"2023-02-15T04:21:14.338238Z","iopub.status.idle":"2023-02-15T05:32:02.482659Z","shell.execute_reply.started":"2023-02-15T04:21:14.338213Z","shell.execute_reply":"2023-02-15T05:32:02.481205Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"2023-02-15 04:21:15.073849: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 04:21:15.073914: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTensorflow Version: 2.4.1\nPython Version: 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \n[GCC 9.3.0]\nCompute dtype: float32\nVariable dtype: float32\nRunning on TPU  grpc://10.0.0.2:8470\n2023-02-15 04:21:18.170415: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 04:21:18.170648: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 04:21:18.170674: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2023-02-15 04:21:18.170707: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (375a1fcc8acc): /proc/driver/nvidia/version does not exist\n2023-02-15 04:21:18.171198: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-15 04:21:18.171521: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 04:21:18.172022: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 04:21:18.176245: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 04:21:18.176283: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30170}\n2023-02-15 04:21:18.189291: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 04:21:18.189474: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30170}\n2023-02-15 04:21:18.189885: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30170\nN_REPLICAS: 8, IS_TPU: True\nBATCH_SIZE: 64\n2023-02-15 04:21:24.461195: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\nFound 100 TFRecords\n2023-02-15 04:21:24.783582: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n# TFRECORDS_TRAIN: 83, # TFRECORDS_VAL: 20\nTRAIN_STEPS_PER_EPOCH: 710, VAL_STEPS_PER_EPOCH: 171\nimage shape: (64, 1344, 768, 1), y_batch shape: (64,)\nimage dtype: uint8, y_batch dtype: <dtype: 'int64'>\nimage min: 0.00, max: 255.00\nimage shape: (64, 1344, 768, 1), labels shape: (64,), image dtype: <dtype: 'uint8'>, labels dtype: <dtype: 'int64'>\nepoch 0 took: 0.83 sec, mean step duration: 83.4ms, images/s: 767\n2023-02-15 04:21:28.525919: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 0\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676434888.522513491\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 0\",\"grpc_status\":3}\nepoch 1 took: 0.7 sec, mean step duration: 70.4ms, images/s: 909\nepoch 2 took: 0.94 sec, mean step duration: 93.6ms, images/s: 683\nCompute dtype: float32\nVariable dtype: float32\n>>>> Load pretrained from: /root/.keras/models/convnext_v2_tiny_384_imagenet21k-ft1k.h5\n171/171 [==============================] - 56s 206ms/step - loss: 0.6936 - pF1: 0.0421 - f1_score: 0.0449 - precision: 0.0235 - recall: 0.5132 - auc: 0.5351 - binary_accuracy: 0.5370\nEpoch 1/10\nlearning rate: 4.00e-05, weight decay: 3.20e-07\n710/710 [==============================] - ETA: 0s - loss: 0.3817 - pF1: 0.3268 - f1_score: 0.5794 - precision: 0.8309 - recall: 0.4447 - auc: 0.7479 - binary_accuracy: 0.95812023-02-15 04:30:25.267209: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 3\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676435425.267045174\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 3\",\"grpc_status\":3}\n710/710 [==============================] - 464s 556ms/step - loss: 0.3817 - pF1: 0.3268 - f1_score: 0.5794 - precision: 0.8309 - recall: 0.4447 - auc: 0.7479 - binary_accuracy: 0.9581 - val_loss: 0.1592 - val_pF1: 0.0383 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6220 - val_binary_accuracy: 0.9801\nEpoch 2/10\nlearning rate: 3.90e-05, weight decay: 3.12e-07\n710/710 [==============================] - 394s 555ms/step - loss: 0.3303 - pF1: 0.5164 - f1_score: 0.7617 - precision: 0.9309 - recall: 0.6445 - auc: 0.8643 - binary_accuracy: 0.9625 - val_loss: 0.1110 - val_pF1: 0.0692 - val_f1_score: 0.0424 - val_precision: 0.2778 - val_recall: 0.0229 - val_auc: 0.7492 - val_binary_accuracy: 0.9793\nEpoch 3/10\nlearning rate: 3.62e-05, weight decay: 2.89e-07\n710/710 [==============================] - 394s 556ms/step - loss: 0.2826 - pF1: 0.4349 - f1_score: 0.6508 - precision: 0.7659 - recall: 0.5658 - auc: 0.8804 - binary_accuracy: 0.9605 - val_loss: 0.1278 - val_pF1: 0.1466 - val_f1_score: 0.2950 - val_precision: 0.2533 - val_recall: 0.3532 - val_auc: 0.8174 - val_binary_accuracy: 0.9664\nEpoch 4/10\nlearning rate: 3.18e-05, weight decay: 2.54e-07\n710/710 [==============================] - 395s 556ms/step - loss: 0.2236 - pF1: 0.6446 - f1_score: 0.8100 - precision: 0.8507 - recall: 0.7730 - auc: 0.9502 - binary_accuracy: 0.9660 - val_loss: 0.1152 - val_pF1: 0.1466 - val_f1_score: 0.2820 - val_precision: 0.2675 - val_recall: 0.2982 - val_auc: 0.8281 - val_binary_accuracy: 0.9697\nEpoch 5/10\nlearning rate: 2.62e-05, weight decay: 2.09e-07\n710/710 [==============================] - 394s 555ms/step - loss: 0.1772 - pF1: 0.7029 - f1_score: 0.8391 - precision: 0.8586 - recall: 0.8204 - auc: 0.9703 - binary_accuracy: 0.9707 - val_loss: 0.1038 - val_pF1: 0.1752 - val_f1_score: 0.2973 - val_precision: 0.2920 - val_recall: 0.3028 - val_auc: 0.8155 - val_binary_accuracy: 0.9715\nEpoch 6/10\nlearning rate: 2.00e-05, weight decay: 1.60e-07\n710/710 [==============================] - 394s 555ms/step - loss: 0.1430 - pF1: 0.6593 - f1_score: 0.8062 - precision: 0.8049 - recall: 0.8076 - auc: 0.9756 - binary_accuracy: 0.9748 - val_loss: 0.1135 - val_pF1: 0.1910 - val_f1_score: 0.2835 - val_precision: 0.2434 - val_recall: 0.3394 - val_auc: 0.8137 - val_binary_accuracy: 0.9658\nEpoch 7/10\nlearning rate: 1.38e-05, weight decay: 1.11e-07\n710/710 [==============================] - 394s 555ms/step - loss: 0.1001 - pF1: 0.8148 - f1_score: 0.9060 - precision: 0.9018 - recall: 0.9102 - auc: 0.9912 - binary_accuracy: 0.9824 - val_loss: 0.1063 - val_pF1: 0.2509 - val_f1_score: 0.2957 - val_precision: 0.2810 - val_recall: 0.3119 - val_auc: 0.7869 - val_binary_accuracy: 0.9704\nEpoch 8/10\nlearning rate: 8.24e-06, weight decay: 6.60e-08\n710/710 [==============================] - 394s 556ms/step - loss: 0.0735 - pF1: 0.8422 - f1_score: 0.9190 - precision: 0.9119 - recall: 0.9262 - auc: 0.9944 - binary_accuracy: 0.9867 - val_loss: 0.1040 - val_pF1: 0.2364 - val_f1_score: 0.2981 - val_precision: 0.3131 - val_recall: 0.2844 - val_auc: 0.7778 - val_binary_accuracy: 0.9733\nEpoch 9/10\nlearning rate: 3.82e-06, weight decay: 3.06e-08\n710/710 [==============================] - 395s 556ms/step - loss: 0.0668 - pF1: 0.8474 - f1_score: 0.9199 - precision: 0.9127 - recall: 0.9271 - auc: 0.9943 - binary_accuracy: 0.9876 - val_loss: 0.1076 - val_pF1: 0.2559 - val_f1_score: 0.3040 - val_precision: 0.3631 - val_recall: 0.2615 - val_auc: 0.7602 - val_binary_accuracy: 0.9761\nEpoch 10/10\nlearning rate: 9.79e-07, weight decay: 7.83e-09\n710/710 [==============================] - 394s 555ms/step - loss: 0.0597 - pF1: 0.8868 - f1_score: 0.9430 - precision: 0.9363 - recall: 0.9498 - auc: 0.9961 - binary_accuracy: 0.9893 - val_loss: 0.1056 - val_pF1: 0.2625 - val_f1_score: 0.3101 - val_precision: 0.3550 - val_recall: 0.2752 - val_auc: 0.7722 - val_binary_accuracy: 0.9756\n  0%|          | 0/171 [00:00<?, ?it/s]\n2023-02-15 05:31:53.376250: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 189718, Output num: 1\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676439113.376153869\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 189718, Output num: 1\",\"grpc_status\":3}\n  0%|          | 0/101 [00:00<?, ?it/s]\nconvext_m1.py:510: RuntimeWarning: invalid value encountered in long_scalars\n  c_precision = ctp / (ctp + cfp)\nBest Threshold 0.56.\npF1 Score: 0.32.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile convext_m2.py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom tqdm.notebook import tqdm\nfrom multiprocessing import cpu_count\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom keras_cv_attention_models import convnext\n\nimport os\nimport time\nimport pickle\nimport math\nimport random\nimport sys\nimport cv2\nimport gc\nimport datetime\n\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Python Version: {sys.version}')\n\n# Save Versions\n\nnow = datetime.datetime.now().strftime(\"%d-%b-%Y %H-%M-%S\")\nnp.save(now, np.array([now]))\n\n# Mixed Precision Policy\n\n# float32 or mixed_float16 (mixed precision: compute float16, variable float32)\n# TPU is fast enough and has enough memory to use float32\npolicy = tf.keras.mixed_precision.Policy('float32')\ntf.keras.mixed_precision.set_global_policy(policy)\n\nprint(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\nprint(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n\n# Matplotlib Config\n\n# MatplotLib Global Settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.labelsize'] = 18\nmpl.rcParams['axes.titlesize'] = 24\n\n# Config\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    IS_TPU = True\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    IS_TPU = False\n    STRATEGY = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nN_REPLICAS = STRATEGY.num_replicas_in_sync\nprint(f'N_REPLICAS: {N_REPLICAS}, IS_TPU: {IS_TPU}')\n\n#/kaggle/input/rsna-tfrecords-768x1344-dataset\n\n# For TPU's the dataset needs to be stored in Google Cloud\n# Retrieve the Google Cloud location of the dataset\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('rsna-tfrecords-768x1344-dataset2')\n\nSEED = 43\nDEBUG = False\nVERSION = 'convext_m2'\n# Image dimensions\nIMG_HEIGHT = 1344\nIMG_WIDTH = 768\nN_CHANNELS = 1\nINPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 1)\nN_SAMPLES_TFRECORDS = 548\n\n# Peak Learning Rate\nLR_MAX = 5e-6 * N_REPLICAS\nWD_RATIO = 0.008\n\nN_WARMUP_EPOCHS = 0\nN_EPOCHS = 10\n\n# Batch size\nBATCH_SIZE = 8 * N_REPLICAS\n\n# Is Interactive Flag and COrresponding Verbosity Method\nIS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\nVERBOSE = 1 if IS_INTERACTIVE else 2\n\n# Tensorflow AUTO flag\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(f'BATCH_SIZE: {BATCH_SIZE}')\n\n# Seed\n\n# Seed all random number generators\ndef seed_everything(seed=SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()\n\n# Train\n\n# Train DataFrame\ntrain = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n\n\n# Utility Functions\n\n# short Tensorflow randin integer function\ndef tf_rand_int(minval, maxval, dtype=tf.int64):\n    minval = tf.cast(minval, dtype)\n    maxval = tf.cast(maxval, dtype)\n    return tf.random.uniform(shape=(), minval=minval, maxval=maxval, dtype=dtype)\n\n# chance of 1 in k\ndef one_in(k):\n    return 0 == tf_rand_int(0, k)\n\n# Dataset\n\n# Function to benchmark the dataset\ndef benchmark_dataset(dataset, num_epochs=3, n_steps_per_epoch=10, bs=BATCH_SIZE):\n    start_time = time.perf_counter()\n    for epoch_num in range(num_epochs):\n        for idx, (inputs, labels) in enumerate(dataset.take(n_steps_per_epoch + 1)):\n            if idx == 0:\n                epoch_start = time.perf_counter()\n            elif idx == 1 and epoch_num == 0:\n                image = inputs['image']\n                print(f'image shape: {image.shape}, labels shape: {labels.shape}, image dtype: {image.dtype}, labels dtype: {labels.dtype}')\n            else:\n                pass\n        \n        epoch_t = time.perf_counter() - epoch_start\n        mean_step_t = round(epoch_t / n_steps_per_epoch * 1000, 1)\n        n_imgs_per_s = int(1 / (mean_step_t / 1000) * bs)\n        print(f'epoch {epoch_num} took: {round(epoch_t, 2)} sec, mean step duration: {mean_step_t}ms, images/s: {n_imgs_per_s}')\n\n# Plots a batch of images\ndef show_batch(dataset, n_rows=16, n_cols=4):\n    inputs, targets = next(iter(dataset))\n    images = inputs['image'].numpy().squeeze()\n    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols*4, n_rows*7))\n    for r in range(n_rows):\n        for c in range(n_cols):\n            idx = r * n_cols + c\n            # Image\n            img = images[idx]\n            axes[r, c].imshow(img)\n            # Target\n            target = targets[idx]\n            axes[r, c].set_title(f'target: {target}', fontsize=16, pad=5)\n        \n    plt.show()\n\n# Decodes the TFRecords\ndef decode_image(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64),\n        'patient_id': tf.io.FixedLenFeature([], tf.int64),\n    })\n    \n    # Decode PNG Image\n    image = tf.io.decode_jpeg(features['image'], channels=N_CHANNELS)\n    # Explicit reshape needed for TPU\n    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, N_CHANNELS])\n\n    target = features['target']\n    \n    return { 'image': image }, target\n\ndef augment_image(X, y):\n    image = X['image']\n    \n    # Random Brightness\n    image = tf.image.random_brightness(image, 0.10)\n    \n    # Random Contrast\n    image = tf.image.random_contrast(image, 0.90, 1.10)\n    \n    # Random JPEG Quality\n    image = tf.image.random_jpeg_quality(image, 75, 100)\n    \n    # Random crop image with maximum of 10%\n    ratio = tf.random.uniform([], 0.75, 1.00)\n    img_height_crop = tf.cast(ratio * IMG_HEIGHT, tf.int32)\n    img_width_crop = tf.cast(ratio * IMG_WIDTH, tf.int32)\n    # Random offset for crop\n    img_height_offset = tf_rand_int(0, IMG_HEIGHT - img_height_crop)\n    img_width_offset = 0\n    # Crop And Resize\n    image = tf.slice(image, [img_height_offset, img_width_offset, 0], [img_height_crop, img_width_crop, N_CHANNELS])\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.BILINEAR)\n    # Clip pixel values in range [0,255] to prevent underflow/overflow\n    image = tf.clip_by_value(image, 0, 255)\n    image = tf.cast(image, tf.uint8)\n    \n    return { 'image': image }, y\n\n# Undersample majority class (0/negative) by randomly dropping them\ndef undersample_majority(X, y):\n    # Filter 2/3 of negative samples to upsample positive samples by a factor 3\n    return y == 1 or tf.random.uniform([]) > 0.40\n\n# TFRecord file paths\nTFRECORDS_FILE_PATHS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH}/*.tfrecords'))\nprint(f'Found {len(TFRECORDS_FILE_PATHS)} TFRecords')\n\n# Train Test Split\nTFRECORDS_TRAIN, TFRECORDS_VAL = train_test_split(TFRECORDS_FILE_PATHS, train_size=0.80, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\nTFRECORDS_TRAIN2, TFRECORDS_VAL2 = train_test_split(TFRECORDS_TRAIN, train_size=0.75, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN2)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL2)}')\nTFRECORDS_TRAIN3, TFRECORDS_VAL3 = train_test_split(TFRECORDS_TRAIN2, train_size=0.67, random_state=SEED, shuffle=True)\n\nTFRECORDS_TRAIN = [x  for x in TFRECORDS_FILE_PATHS if x not in TFRECORDS_VAL3]\nTFRECORDS_VAL = TFRECORDS_VAL3\nGCS_DS_PATH_POS = KaggleDatasets().get_gcs_path('rsna-extra-positive-tfrecord')\nTFRECORDS_FILE_PATHS_POS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH_POS}/*.tfrecords'))\n\nTFRECORDS_TRAIN = TFRECORDS_TRAIN+TFRECORDS_FILE_PATHS_POS\nprint(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\ndef get_dataset(tfrecords, bs=BATCH_SIZE, val=False, debug=True):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    \n    # Initialize dataset with TFRecords\n    dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=AUTO, compression_type='GZIP')\n    \n    # Decode mapping\n    dataset = dataset.map(decode_image, num_parallel_calls=AUTO)\n\n    if not val:\n        dataset = dataset.filter(undersample_majority)\n        dataset = dataset.map(augment_image, num_parallel_calls=AUTO)\n        dataset = dataset.with_options(ignore_order)\n        if not debug:\n            dataset = dataset.shuffle(1024)\n        dataset = dataset.repeat()        \n\n    dataset = dataset.batch(bs, drop_remainder=not val)\n    dataset = dataset.prefetch(AUTO)\n    \n    return dataset\n\n# Get Train/Validation datasets\ntrain_dataset = get_dataset(TFRECORDS_TRAIN, val=False, debug=False)\nval_dataset = get_dataset(TFRECORDS_VAL, val=True, debug=False)\n\nTRAIN_STEPS_PER_EPOCH = len(TFRECORDS_TRAIN) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nVAL_STEPS_PER_EPOCH = len(TFRECORDS_VAL) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nprint(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}, VAL_STEPS_PER_EPOCH: {VAL_STEPS_PER_EPOCH}')\n\n# Sanity check, image and label statistics\nX_batch, y_batch = next(iter(get_dataset(TFRECORDS_TRAIN, val=False)))\nimage = X_batch['image'].numpy()\nprint(f'image shape: {image.shape}, y_batch shape: {y_batch.shape}')\nprint(f'image dtype: {image.dtype}, y_batch dtype: {y_batch.dtype}')\nprint(f'image min: {image.min():.2f}, max: {image.max():.2f}')\n\n# Benchmark Dataset\nbenchmark_dataset(get_dataset(TFRECORDS_TRAIN, val=False))\n\n# Show what we will be training on\nshow_batch(get_dataset(TFRECORDS_TRAIN, bs=64, val=False))\n\n# Tensorflow custom metric is just a conventional class object\nclass pF1(tf.keras.metrics.Metric):\n    # Initialize properties\n    def __init__(self, name='pF1', **kwargs):\n        super(pF1, self).__init__(name=name, **kwargs)\n        self.tc = self.add_weight(name='tc', initializer='zeros')\n        self.tp = self.add_weight(name='tp', initializer='zeros')\n        self.fp = self.add_weight(name='fp', initializer='zeros')\n\n    # Update state called on each batch with true and predicted labels\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.tc.assign_add(tf.cast(tf.reduce_sum(y_true), tf.float32))\n        self.tp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 1])), tf.float32))\n        self.fp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 0])), tf.float32))\n\n    # Result function is called to obtain result which is printed in progress bar\n    def result(self):\n        if self.tc == 0 or (self.tp + self.fp) == 0:\n            return 0.0\n        else:\n            precision = self.tp / (self.tp + self.fp)\n            recall = self.tp / (self.tc)\n            return 2 * (precision * recall) / (precision + recall)\n\n    # Reset state is called after each epoch to start fresh each epoch\n    def reset_state(self):\n        self.tc.assign(0)\n        self.tp.assign(0)\n        self.fp.assign(0)\n\ndef normalize(image):\n    # Repeat channels to create 3 channel images required by pretrained ConvNextV2 models\n    image = tf.repeat(image, repeats=3, axis=3)\n    # Cast to float 32\n    image = tf.cast(image, tf.float32)\n    # Normalize with respect to ImageNet mean/std\n    image = tf.keras.applications.imagenet_utils.preprocess_input(image, mode='torch')\n\n    return image\n\ndef get_model():\n    # Verify Mixed Policy Settings\n    print(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\n    print(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n    \n    with STRATEGY.scope():\n        # Set seed for deterministic weights initialization\n        seed_everything()\n        \n        # Inputs, note the names are equal to the dictionary keys in the dataset\n        image = tf.keras.layers.Input(INPUT_SHAPE, name='image', dtype=tf.uint8)\n        \n        # Normalize Input\n        image_norm = normalize(image)\n\n        # CNN Prediction in range [0,1]\n        x = convnext.ConvNeXtV2Tiny(\n            input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n            pretrained='imagenet21k-ft1k',\n            num_classes=0,\n        )(image_norm)\n        \n        # Average Pooling BxHxWxC -> BxC\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        # Dropout to prevent Overfitting\n        x = tf.keras.layers.Dropout(0.30)(x)\n        # Output value between [0, 1] using Sigmoid function\n        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n        # We will use the famous AdamW optimizer for fast learning with weight decay\n        optimizer = tfa.optimizers.AdamW(learning_rate=LR_MAX, weight_decay=LR_MAX*WD_RATIO, epsilon=1e-6)\n\n        # Loss\n        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n        \n        # Metrics\n        metrics = [\n            pF1(),\n            tfa.metrics.F1Score(num_classes=1, threshold=0.50),\n            tf.keras.metrics.Precision(),\n            tf.keras.metrics.Recall(),\n            tf.keras.metrics.AUC(),\n            tf.keras.metrics.BinaryAccuracy(),\n        ]\n\n        model = tf.keras.models.Model(inputs=image, outputs=outputs)\n        \n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n        return model\n\n# Pretrained File Path: '/kaggle/input/sartorius-training-dataset/model.h5'\ntf.keras.backend.clear_session()\n# enable XLA optmizations\ntf.config.optimizer.set_jit(True)\n\nmodel = get_model()\n\n# Weight Initilization\n\n# Validation metric on initialized model\n_ = model.evaluate(\n        get_dataset(TFRECORDS_VAL, val=True),\n        verbose=VERBOSE,\n        steps=VAL_STEPS_PER_EPOCH,\n    )\n\n# Learning rate scheduler with logaritmic warmup and cosine decay\ndef lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n    \n    if current_step < num_warmup_steps:\n        return lr_max * 0.10 ** (num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n\n# Plot the learning rate scheduler\ndef plot_lr_schedule(lr_schedule, epochs):\n    fig = plt.figure(figsize=(20, 10))\n    plt.plot([None] + lr_schedule + [None])\n    # X Labels\n    x = np.arange(1, epochs + 1)\n    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n    plt.xlim([1, epochs])\n    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n    \n    # Increase y-limit for better readability\n    plt.ylim([0, max(lr_schedule) * 1.1])\n    \n    # Title\n    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n    \n    # Plot Learning Rates\n    for x, val in enumerate(lr_schedule):\n        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n            if x < len(lr_schedule) - 1:\n                if lr_schedule[x - 1] < val:\n                    ha = 'right'\n                else:\n                    ha = 'left'\n            elif x == 0:\n                ha = 'right'\n            else:\n                ha = 'left'\n            plt.plot(x + 1, val, 'o', color='black');\n            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n    \n    plt.xlabel('Epoch', size=16, labelpad=5)\n    plt.ylabel('Learning Rate', size=16, labelpad=5)\n    plt.grid()\n    plt.show()\n\n# Learning rate for encoder\nLR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\nplot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n\n# Learning Rate Callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n\n# Weight Decay Callback\n\n# Tensorflow Learning Rate Scheduler does not update weight decay, need to do it manually in a custom callback\nclass WeightDecayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, wd_ratio=WD_RATIO):\n        self.step_counter = 0\n        self.wd_ratio = wd_ratio\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')\n\n# Train model on TPU!\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=f'best_{VERSION}.h5',\n    save_weights_only=True,\n    monitor='val_pF1',\n    mode='max',\n    save_best_only=True)\nhistory = model.fit(\n        train_dataset,\n        steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n        validation_data = val_dataset,\n        epochs = N_EPOCHS,\n        verbose = VERBOSE,\n        callbacks = [\n            model_checkpoint_callback,\n            lr_callback,\n            WeightDecayCallback(),\n        ],\n        class_weight = {\n            0: 1.0,\n            1: 3.0,\n        },\n    )\n\n# Save model weights for inference\nmodel.save_weights(f'model_{VERSION}.h5')\n\n# F1 By Threshold\n\n# Get true labels and predictions for validation set\ny_true_val = []\ny_pred_val = []\nfor X_batch, y_batch in tqdm(get_dataset(TFRECORDS_VAL, val=True), total=VAL_STEPS_PER_EPOCH):\n    y_true_val += y_batch.numpy().tolist()\n    y_pred_val += model.predict_on_batch(X_batch).squeeze().tolist()\n\n# source: https://www.kaggle.com/code/sohier/probabilistic-f-score\n# Competition Leaderboard Metric\ndef pfbeta(labels, predictions, beta=1):\n    y_true_count = 0\n    ctp = 0\n    cfp = 0\n\n    for idx in range(len(labels)):\n        prediction = min(max(predictions[idx], 0), 1)\n        if (labels[idx]):\n            y_true_count += 1\n            ctp += prediction\n        else:\n            cfp += prediction\n\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp)\n    c_recall = ctp / y_true_count\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n        return result\n    else:\n        return 0\n\n# Plot pF1 by threshold plot to find best threshold\npf1_by_threshold = []\nthresholds = np.arange(0, 1.01, 0.01)\nfor t in tqdm(thresholds):\n    # Compute pF1 for each threshold\n    pf1_by_threshold.append(\n        pfbeta(y_true_val, y_pred_val > t)\n    )\n    \nplt.figure(figsize=(15,8))\nplt.title('F1 By Threshold', size=24)\nplt.plot(pf1_by_threshold, label='F1 Score')\n\n# Best threshold and pF1 score\narg_max = np.argmax(pf1_by_threshold)\nval_max = np.max(pf1_by_threshold)\nthreshold_best = thresholds[arg_max]\nplt.scatter(arg_max, val_max, color='red', label=f'Best Threshold {threshold_best:.2f}, pF1 Score: {val_max:.2f}')\n\n# Plot pF1 by Threshold\nplt.xticks(np.arange(0, 110, 10), [f'{t:.2f}' for t in np.arange(0, 1.1, 0.1)])\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.xlim(0, 100)\nplt.ylim(0, 1)\nplt.xlabel('Threshold')\nplt.ylabel('pF1 Score')\nplt.legend(fontsize=12)\nplt.grid()\nplt.show()\nprint(f'Best Threshold {threshold_best:.2f}.')\n\nprint(f'pF1 Score: {val_max:.2f}.')\n\n# Training History","metadata":{"execution":{"iopub.status.busy":"2023-02-15T05:32:02.486626Z","iopub.execute_input":"2023-02-15T05:32:02.487005Z","iopub.status.idle":"2023-02-15T05:32:02.504932Z","shell.execute_reply.started":"2023-02-15T05:32:02.486944Z","shell.execute_reply":"2023-02-15T05:32:02.504015Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing convext_m2.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python convext_m2.py","metadata":{"execution":{"iopub.status.busy":"2023-02-15T05:32:02.506248Z","iopub.execute_input":"2023-02-15T05:32:02.507277Z","iopub.status.idle":"2023-02-15T06:42:34.559153Z","shell.execute_reply.started":"2023-02-15T05:32:02.507245Z","shell.execute_reply":"2023-02-15T06:42:34.558233Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"2023-02-15 05:32:03.606091: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 05:32:03.606182: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTensorflow Version: 2.4.1\nPython Version: 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \n[GCC 9.3.0]\nCompute dtype: float32\nVariable dtype: float32\nRunning on TPU  grpc://10.0.0.2:8470\n2023-02-15 05:32:07.317400: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 05:32:07.317774: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 05:32:07.317818: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2023-02-15 05:32:07.317855: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (375a1fcc8acc): /proc/driver/nvidia/version does not exist\n2023-02-15 05:32:07.318492: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-15 05:32:07.318877: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 05:32:07.319640: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 05:32:07.324737: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 05:32:07.324817: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30287}\n2023-02-15 05:32:07.338715: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 05:32:07.338925: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30287}\n2023-02-15 05:32:07.339779: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30287\nN_REPLICAS: 8, IS_TPU: True\nBATCH_SIZE: 64\n2023-02-15 05:32:12.773829: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\nFound 100 TFRecords\n2023-02-15 05:32:13.146131: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n# TFRECORDS_TRAIN: 83, # TFRECORDS_VAL: 20\nTRAIN_STEPS_PER_EPOCH: 710, VAL_STEPS_PER_EPOCH: 171\nimage shape: (64, 1344, 768, 1), y_batch shape: (64,)\nimage dtype: uint8, y_batch dtype: <dtype: 'int64'>\nimage min: 0.00, max: 255.00\nimage shape: (64, 1344, 768, 1), labels shape: (64,), image dtype: <dtype: 'uint8'>, labels dtype: <dtype: 'int64'>\nepoch 0 took: 0.78 sec, mean step duration: 78.0ms, images/s: 820\n2023-02-15 05:32:16.258827: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 0\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676439136.255405328\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 0\",\"grpc_status\":3}\nepoch 1 took: 0.81 sec, mean step duration: 80.8ms, images/s: 792\nepoch 2 took: 0.82 sec, mean step duration: 81.7ms, images/s: 783\nCompute dtype: float32\nVariable dtype: float32\n>>>> Load pretrained from: /root/.keras/models/convnext_v2_tiny_384_imagenet21k-ft1k.h5\n171/171 [==============================] - 56s 207ms/step - loss: 0.5540 - pF1: 0.0529 - f1_score: 0.0501 - precision: 0.0277 - recall: 0.2677 - auc: 0.5478 - binary_accuracy: 0.7275\nEpoch 1/10\nlearning rate: 4.00e-05, weight decay: 3.20e-07\n710/710 [==============================] - ETA: 0s - loss: 0.3567 - pF1: 0.3452 - f1_score: 0.5926 - precision: 0.8001 - recall: 0.4705 - auc: 0.7754 - binary_accuracy: 0.95942023-02-15 05:41:11.664153: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 4\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676439671.664059392\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 4\",\"grpc_status\":3}\n710/710 [==============================] - 463s 553ms/step - loss: 0.3567 - pF1: 0.3452 - f1_score: 0.5926 - precision: 0.8001 - recall: 0.4705 - auc: 0.7754 - binary_accuracy: 0.9594 - val_loss: 0.2414 - val_pF1: 0.0630 - val_f1_score: 0.1365 - val_precision: 0.3286 - val_recall: 0.0861 - val_auc: 0.7072 - val_binary_accuracy: 0.9734\nEpoch 2/10\nlearning rate: 3.90e-05, weight decay: 3.12e-07\n710/710 [==============================] - 394s 555ms/step - loss: 0.2734 - pF1: 0.5742 - f1_score: 0.7808 - precision: 0.8758 - recall: 0.7044 - auc: 0.9156 - binary_accuracy: 0.9638 - val_loss: 0.1653 - val_pF1: 0.1537 - val_f1_score: 0.2838 - val_precision: 0.2208 - val_recall: 0.3970 - val_auc: 0.8383 - val_binary_accuracy: 0.9511\nEpoch 3/10\nlearning rate: 3.62e-05, weight decay: 2.89e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.2300 - pF1: 0.5057 - f1_score: 0.7019 - precision: 0.7556 - recall: 0.6553 - auc: 0.9236 - binary_accuracy: 0.9649 - val_loss: 0.1898 - val_pF1: 0.1512 - val_f1_score: 0.2886 - val_precision: 0.2072 - val_recall: 0.4757 - val_auc: 0.8518 - val_binary_accuracy: 0.9428\nEpoch 4/10\nlearning rate: 3.18e-05, weight decay: 2.54e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.1786 - pF1: 0.7017 - f1_score: 0.8403 - precision: 0.8555 - recall: 0.8256 - auc: 0.9675 - binary_accuracy: 0.9713 - val_loss: 0.1055 - val_pF1: 0.2646 - val_f1_score: 0.3477 - val_precision: 0.3470 - val_recall: 0.3483 - val_auc: 0.8444 - val_binary_accuracy: 0.9681\nEpoch 5/10\nlearning rate: 2.62e-05, weight decay: 2.09e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.1358 - pF1: 0.7621 - f1_score: 0.8709 - precision: 0.8703 - recall: 0.8715 - auc: 0.9829 - binary_accuracy: 0.9762 - val_loss: 0.1026 - val_pF1: 0.2768 - val_f1_score: 0.4023 - val_precision: 0.4077 - val_recall: 0.3970 - val_auc: 0.8477 - val_binary_accuracy: 0.9712\nEpoch 6/10\nlearning rate: 2.00e-05, weight decay: 1.60e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.0991 - pF1: 0.7516 - f1_score: 0.8639 - precision: 0.8556 - recall: 0.8723 - auc: 0.9860 - binary_accuracy: 0.9827 - val_loss: 0.1182 - val_pF1: 0.2891 - val_f1_score: 0.3419 - val_precision: 0.3357 - val_recall: 0.3483 - val_auc: 0.8110 - val_binary_accuracy: 0.9673\nEpoch 7/10\nlearning rate: 1.38e-05, weight decay: 1.11e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.0715 - pF1: 0.8664 - f1_score: 0.9339 - precision: 0.9313 - recall: 0.9365 - auc: 0.9946 - binary_accuracy: 0.9878 - val_loss: 0.1175 - val_pF1: 0.3035 - val_f1_score: 0.3682 - val_precision: 0.3815 - val_recall: 0.3558 - val_auc: 0.7984 - val_binary_accuracy: 0.9702\nEpoch 8/10\nlearning rate: 8.24e-06, weight decay: 6.60e-08\n710/710 [==============================] - 393s 554ms/step - loss: 0.0498 - pF1: 0.8991 - f1_score: 0.9501 - precision: 0.9445 - recall: 0.9557 - auc: 0.9974 - binary_accuracy: 0.9910 - val_loss: 0.1176 - val_pF1: 0.2895 - val_f1_score: 0.3476 - val_precision: 0.3829 - val_recall: 0.3184 - val_auc: 0.7866 - val_binary_accuracy: 0.9708\nEpoch 9/10\nlearning rate: 3.82e-06, weight decay: 3.06e-08\n710/710 [==============================] - 393s 554ms/step - loss: 0.0433 - pF1: 0.8797 - f1_score: 0.9415 - precision: 0.9366 - recall: 0.9464 - auc: 0.9975 - binary_accuracy: 0.9923 - val_loss: 0.1309 - val_pF1: 0.3422 - val_f1_score: 0.3802 - val_precision: 0.4240 - val_recall: 0.3446 - val_auc: 0.7638 - val_binary_accuracy: 0.9726\nEpoch 10/10\nlearning rate: 9.79e-07, weight decay: 7.83e-09\n710/710 [==============================] - 394s 555ms/step - loss: 0.0389 - pF1: 0.9241 - f1_score: 0.9620 - precision: 0.9583 - recall: 0.9657 - auc: 0.9986 - binary_accuracy: 0.9930 - val_loss: 0.1262 - val_pF1: 0.3319 - val_f1_score: 0.3640 - val_precision: 0.4250 - val_recall: 0.3184 - val_auc: 0.7606 - val_binary_accuracy: 0.9729\n  0%|          | 0/171 [00:00<?, ?it/s]\n2023-02-15 06:42:25.822438: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 188926, Output num: 1\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676443345.822292235\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 188926, Output num: 1\",\"grpc_status\":3}\n  0%|          | 0/101 [00:00<?, ?it/s]\nconvext_m2.py:512: RuntimeWarning: invalid value encountered in long_scalars\n  c_precision = ctp / (ctp + cfp)\nBest Threshold 0.62.\npF1 Score: 0.38.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile convext_m3.py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom tqdm.notebook import tqdm\nfrom multiprocessing import cpu_count\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom keras_cv_attention_models import convnext\n\nimport os\nimport time\nimport pickle\nimport math\nimport random\nimport sys\nimport cv2\nimport gc\nimport datetime\n\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Python Version: {sys.version}')\n\n# Save Versions\n\nnow = datetime.datetime.now().strftime(\"%d-%b-%Y %H-%M-%S\")\nnp.save(now, np.array([now]))\n\n# Mixed Precision Policy\n\n# float32 or mixed_float16 (mixed precision: compute float16, variable float32)\n# TPU is fast enough and has enough memory to use float32\npolicy = tf.keras.mixed_precision.Policy('float32')\ntf.keras.mixed_precision.set_global_policy(policy)\n\nprint(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\nprint(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n\n# Matplotlib Config\n\n# MatplotLib Global Settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.labelsize'] = 18\nmpl.rcParams['axes.titlesize'] = 24\n\n# Config\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    IS_TPU = True\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    IS_TPU = False\n    STRATEGY = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nN_REPLICAS = STRATEGY.num_replicas_in_sync\nprint(f'N_REPLICAS: {N_REPLICAS}, IS_TPU: {IS_TPU}')\n\n#/kaggle/input/rsna-tfrecords-768x1344-dataset\n\n# For TPU's the dataset needs to be stored in Google Cloud\n# Retrieve the Google Cloud location of the dataset\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('rsna-tfrecords-768x1344-dataset2')\n\nSEED = 43\nDEBUG = False\nVERSION = 'convext_m3'\n# Image dimensions\nIMG_HEIGHT = 1344\nIMG_WIDTH = 768\nN_CHANNELS = 1\nINPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 1)\nN_SAMPLES_TFRECORDS = 548\n\n# Peak Learning Rate\nLR_MAX = 5e-6 * N_REPLICAS\nWD_RATIO = 0.008\n\nN_WARMUP_EPOCHS = 0\nN_EPOCHS = 10\n\n# Batch size\nBATCH_SIZE = 8 * N_REPLICAS\n\n# Is Interactive Flag and COrresponding Verbosity Method\nIS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\nVERBOSE = 1 if IS_INTERACTIVE else 2\n\n# Tensorflow AUTO flag\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(f'BATCH_SIZE: {BATCH_SIZE}')\n\n# Seed\n\n# Seed all random number generators\ndef seed_everything(seed=SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()\n\n# Train\n\n# Train DataFrame\ntrain = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n\n\n# Utility Functions\n\n# short Tensorflow randin integer function\ndef tf_rand_int(minval, maxval, dtype=tf.int64):\n    minval = tf.cast(minval, dtype)\n    maxval = tf.cast(maxval, dtype)\n    return tf.random.uniform(shape=(), minval=minval, maxval=maxval, dtype=dtype)\n\n# chance of 1 in k\ndef one_in(k):\n    return 0 == tf_rand_int(0, k)\n\n# Dataset\n\n# Function to benchmark the dataset\ndef benchmark_dataset(dataset, num_epochs=3, n_steps_per_epoch=10, bs=BATCH_SIZE):\n    start_time = time.perf_counter()\n    for epoch_num in range(num_epochs):\n        for idx, (inputs, labels) in enumerate(dataset.take(n_steps_per_epoch + 1)):\n            if idx == 0:\n                epoch_start = time.perf_counter()\n            elif idx == 1 and epoch_num == 0:\n                image = inputs['image']\n                print(f'image shape: {image.shape}, labels shape: {labels.shape}, image dtype: {image.dtype}, labels dtype: {labels.dtype}')\n            else:\n                pass\n        \n        epoch_t = time.perf_counter() - epoch_start\n        mean_step_t = round(epoch_t / n_steps_per_epoch * 1000, 1)\n        n_imgs_per_s = int(1 / (mean_step_t / 1000) * bs)\n        print(f'epoch {epoch_num} took: {round(epoch_t, 2)} sec, mean step duration: {mean_step_t}ms, images/s: {n_imgs_per_s}')\n\n# Plots a batch of images\ndef show_batch(dataset, n_rows=16, n_cols=4):\n    inputs, targets = next(iter(dataset))\n    images = inputs['image'].numpy().squeeze()\n    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols*4, n_rows*7))\n    for r in range(n_rows):\n        for c in range(n_cols):\n            idx = r * n_cols + c\n            # Image\n            img = images[idx]\n            axes[r, c].imshow(img)\n            # Target\n            target = targets[idx]\n            axes[r, c].set_title(f'target: {target}', fontsize=16, pad=5)\n        \n    plt.show()\n\n# Decodes the TFRecords\ndef decode_image(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64),\n        'patient_id': tf.io.FixedLenFeature([], tf.int64),\n    })\n    \n    # Decode PNG Image\n    image = tf.io.decode_jpeg(features['image'], channels=N_CHANNELS)\n    # Explicit reshape needed for TPU\n    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, N_CHANNELS])\n\n    target = features['target']\n    \n    return { 'image': image }, target\n\ndef augment_image(X, y):\n    image = X['image']\n    \n    # Random Brightness\n    image = tf.image.random_brightness(image, 0.10)\n    \n    # Random Contrast\n    image = tf.image.random_contrast(image, 0.90, 1.10)\n    \n    # Random JPEG Quality\n    image = tf.image.random_jpeg_quality(image, 75, 100)\n    \n    # Random crop image with maximum of 10%\n    ratio = tf.random.uniform([], 0.75, 1.00)\n    img_height_crop = tf.cast(ratio * IMG_HEIGHT, tf.int32)\n    img_width_crop = tf.cast(ratio * IMG_WIDTH, tf.int32)\n    # Random offset for crop\n    img_height_offset = tf_rand_int(0, IMG_HEIGHT - img_height_crop)\n    img_width_offset = 0\n    # Crop And Resize\n    image = tf.slice(image, [img_height_offset, img_width_offset, 0], [img_height_crop, img_width_crop, N_CHANNELS])\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.BILINEAR)\n    # Clip pixel values in range [0,255] to prevent underflow/overflow\n    image = tf.clip_by_value(image, 0, 255)\n    image = tf.cast(image, tf.uint8)\n    \n    return { 'image': image }, y\n\n# Undersample majority class (0/negative) by randomly dropping them\ndef undersample_majority(X, y):\n    # Filter 2/3 of negative samples to upsample positive samples by a factor 3\n    return y == 1 or tf.random.uniform([]) > 0.40\n\n# TFRecord file paths\nTFRECORDS_FILE_PATHS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH}/*.tfrecords'))\nprint(f'Found {len(TFRECORDS_FILE_PATHS)} TFRecords')\n\n# Train Test Split\nTFRECORDS_TRAIN, TFRECORDS_VAL = train_test_split(TFRECORDS_FILE_PATHS, train_size=0.80, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\nTFRECORDS_TRAIN2, TFRECORDS_VAL2 = train_test_split(TFRECORDS_TRAIN, train_size=0.75, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN2)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL2)}')\nTFRECORDS_TRAIN3, TFRECORDS_VAL3 = train_test_split(TFRECORDS_TRAIN2, train_size=0.67, random_state=SEED, shuffle=True)\nTFRECORDS_TRAIN4, TFRECORDS_VAL4 = train_test_split(TFRECORDS_TRAIN3, train_size=0.5, random_state=SEED, shuffle=True)\n\nTFRECORDS_TRAIN = [x  for x in TFRECORDS_FILE_PATHS if x not in TFRECORDS_VAL4]\nTFRECORDS_VAL = TFRECORDS_VAL4\nGCS_DS_PATH_POS = KaggleDatasets().get_gcs_path('rsna-extra-positive-tfrecord')\nTFRECORDS_FILE_PATHS_POS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH_POS}/*.tfrecords'))\n\nTFRECORDS_TRAIN = TFRECORDS_TRAIN+TFRECORDS_FILE_PATHS_POS\nprint(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\ndef get_dataset(tfrecords, bs=BATCH_SIZE, val=False, debug=True):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    \n    # Initialize dataset with TFRecords\n    dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=AUTO, compression_type='GZIP')\n    \n    # Decode mapping\n    dataset = dataset.map(decode_image, num_parallel_calls=AUTO)\n\n    if not val:\n        dataset = dataset.filter(undersample_majority)\n        dataset = dataset.map(augment_image, num_parallel_calls=AUTO)\n        dataset = dataset.with_options(ignore_order)\n        if not debug:\n            dataset = dataset.shuffle(1024)\n        dataset = dataset.repeat()        \n\n    dataset = dataset.batch(bs, drop_remainder=not val)\n    dataset = dataset.prefetch(AUTO)\n    \n    return dataset\n\n# Get Train/Validation datasets\ntrain_dataset = get_dataset(TFRECORDS_TRAIN, val=False, debug=False)\nval_dataset = get_dataset(TFRECORDS_VAL, val=True, debug=False)\n\nTRAIN_STEPS_PER_EPOCH = len(TFRECORDS_TRAIN) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nVAL_STEPS_PER_EPOCH = len(TFRECORDS_VAL) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nprint(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}, VAL_STEPS_PER_EPOCH: {VAL_STEPS_PER_EPOCH}')\n\n# Sanity check, image and label statistics\nX_batch, y_batch = next(iter(get_dataset(TFRECORDS_TRAIN, val=False)))\nimage = X_batch['image'].numpy()\nprint(f'image shape: {image.shape}, y_batch shape: {y_batch.shape}')\nprint(f'image dtype: {image.dtype}, y_batch dtype: {y_batch.dtype}')\nprint(f'image min: {image.min():.2f}, max: {image.max():.2f}')\n\n# Benchmark Dataset\nbenchmark_dataset(get_dataset(TFRECORDS_TRAIN, val=False))\n\n# Show what we will be training on\nshow_batch(get_dataset(TFRECORDS_TRAIN, bs=64, val=False))\n\n# Tensorflow custom metric is just a conventional class object\nclass pF1(tf.keras.metrics.Metric):\n    # Initialize properties\n    def __init__(self, name='pF1', **kwargs):\n        super(pF1, self).__init__(name=name, **kwargs)\n        self.tc = self.add_weight(name='tc', initializer='zeros')\n        self.tp = self.add_weight(name='tp', initializer='zeros')\n        self.fp = self.add_weight(name='fp', initializer='zeros')\n\n    # Update state called on each batch with true and predicted labels\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.tc.assign_add(tf.cast(tf.reduce_sum(y_true), tf.float32))\n        self.tp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 1])), tf.float32))\n        self.fp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 0])), tf.float32))\n\n    # Result function is called to obtain result which is printed in progress bar\n    def result(self):\n        if self.tc == 0 or (self.tp + self.fp) == 0:\n            return 0.0\n        else:\n            precision = self.tp / (self.tp + self.fp)\n            recall = self.tp / (self.tc)\n            return 2 * (precision * recall) / (precision + recall)\n\n    # Reset state is called after each epoch to start fresh each epoch\n    def reset_state(self):\n        self.tc.assign(0)\n        self.tp.assign(0)\n        self.fp.assign(0)\n\ndef normalize(image):\n    # Repeat channels to create 3 channel images required by pretrained ConvNextV2 models\n    image = tf.repeat(image, repeats=3, axis=3)\n    # Cast to float 32\n    image = tf.cast(image, tf.float32)\n    # Normalize with respect to ImageNet mean/std\n    image = tf.keras.applications.imagenet_utils.preprocess_input(image, mode='torch')\n\n    return image\n\ndef get_model():\n    # Verify Mixed Policy Settings\n    print(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\n    print(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n    \n    with STRATEGY.scope():\n        # Set seed for deterministic weights initialization\n        seed_everything()\n        \n        # Inputs, note the names are equal to the dictionary keys in the dataset\n        image = tf.keras.layers.Input(INPUT_SHAPE, name='image', dtype=tf.uint8)\n        \n        # Normalize Input\n        image_norm = normalize(image)\n\n        # CNN Prediction in range [0,1]\n        x = convnext.ConvNeXtV2Tiny(\n            input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n            pretrained='imagenet21k-ft1k',\n            num_classes=0,\n        )(image_norm)\n        \n        # Average Pooling BxHxWxC -> BxC\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        # Dropout to prevent Overfitting\n        x = tf.keras.layers.Dropout(0.30)(x)\n        # Output value between [0, 1] using Sigmoid function\n        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n        # We will use the famous AdamW optimizer for fast learning with weight decay\n        optimizer = tfa.optimizers.AdamW(learning_rate=LR_MAX, weight_decay=LR_MAX*WD_RATIO, epsilon=1e-6)\n\n        # Loss\n        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n        \n        # Metrics\n        metrics = [\n            pF1(),\n            tfa.metrics.F1Score(num_classes=1, threshold=0.50),\n            tf.keras.metrics.Precision(),\n            tf.keras.metrics.Recall(),\n            tf.keras.metrics.AUC(),\n            tf.keras.metrics.BinaryAccuracy(),\n        ]\n\n        model = tf.keras.models.Model(inputs=image, outputs=outputs)\n        \n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n        return model\n\n# Pretrained File Path: '/kaggle/input/sartorius-training-dataset/model.h5'\ntf.keras.backend.clear_session()\n# enable XLA optmizations\ntf.config.optimizer.set_jit(True)\n\nmodel = get_model()\n\n# Weight Initilization\n\n# Validation metric on initialized model\n_ = model.evaluate(\n        get_dataset(TFRECORDS_VAL, val=True),\n        verbose=VERBOSE,\n        steps=VAL_STEPS_PER_EPOCH,\n    )\n\n# Learning rate scheduler with logaritmic warmup and cosine decay\ndef lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n    \n    if current_step < num_warmup_steps:\n        return lr_max * 0.10 ** (num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n\n# Plot the learning rate scheduler\ndef plot_lr_schedule(lr_schedule, epochs):\n    fig = plt.figure(figsize=(20, 10))\n    plt.plot([None] + lr_schedule + [None])\n    # X Labels\n    x = np.arange(1, epochs + 1)\n    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n    plt.xlim([1, epochs])\n    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n    \n    # Increase y-limit for better readability\n    plt.ylim([0, max(lr_schedule) * 1.1])\n    \n    # Title\n    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n    \n    # Plot Learning Rates\n    for x, val in enumerate(lr_schedule):\n        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n            if x < len(lr_schedule) - 1:\n                if lr_schedule[x - 1] < val:\n                    ha = 'right'\n                else:\n                    ha = 'left'\n            elif x == 0:\n                ha = 'right'\n            else:\n                ha = 'left'\n            plt.plot(x + 1, val, 'o', color='black');\n            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n    \n    plt.xlabel('Epoch', size=16, labelpad=5)\n    plt.ylabel('Learning Rate', size=16, labelpad=5)\n    plt.grid()\n    plt.show()\n\n# Learning rate for encoder\nLR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\nplot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n\n# Learning Rate Callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n\n# Weight Decay Callback\n\n# Tensorflow Learning Rate Scheduler does not update weight decay, need to do it manually in a custom callback\nclass WeightDecayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, wd_ratio=WD_RATIO):\n        self.step_counter = 0\n        self.wd_ratio = wd_ratio\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')\n\n# Train model on TPU!\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=f'best_{VERSION}.h5',\n    save_weights_only=True,\n    monitor='val_pF1',\n    mode='max',\n    save_best_only=True)\nhistory = model.fit(\n        train_dataset,\n        steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n        validation_data = val_dataset,\n        epochs = N_EPOCHS,\n        verbose = VERBOSE,\n        callbacks = [\n            model_checkpoint_callback,\n            lr_callback,\n            WeightDecayCallback(),\n        ],\n        class_weight = {\n            0: 1.0,\n            1: 3.0,\n        },\n    )\n\n# Save model weights for inference\nmodel.save_weights(f'model_{VERSION}.h5')\n\n# F1 By Threshold\n\n# Get true labels and predictions for validation set\ny_true_val = []\ny_pred_val = []\nfor X_batch, y_batch in tqdm(get_dataset(TFRECORDS_VAL, val=True), total=VAL_STEPS_PER_EPOCH):\n    y_true_val += y_batch.numpy().tolist()\n    y_pred_val += model.predict_on_batch(X_batch).squeeze().tolist()\n\n# source: https://www.kaggle.com/code/sohier/probabilistic-f-score\n# Competition Leaderboard Metric\ndef pfbeta(labels, predictions, beta=1):\n    y_true_count = 0\n    ctp = 0\n    cfp = 0\n\n    for idx in range(len(labels)):\n        prediction = min(max(predictions[idx], 0), 1)\n        if (labels[idx]):\n            y_true_count += 1\n            ctp += prediction\n        else:\n            cfp += prediction\n\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp)\n    c_recall = ctp / y_true_count\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n        return result\n    else:\n        return 0\n\n# Plot pF1 by threshold plot to find best threshold\npf1_by_threshold = []\nthresholds = np.arange(0, 1.01, 0.01)\nfor t in tqdm(thresholds):\n    # Compute pF1 for each threshold\n    pf1_by_threshold.append(\n        pfbeta(y_true_val, y_pred_val > t)\n    )\n    \nplt.figure(figsize=(15,8))\nplt.title('F1 By Threshold', size=24)\nplt.plot(pf1_by_threshold, label='F1 Score')\n\n# Best threshold and pF1 score\narg_max = np.argmax(pf1_by_threshold)\nval_max = np.max(pf1_by_threshold)\nthreshold_best = thresholds[arg_max]\nplt.scatter(arg_max, val_max, color='red', label=f'Best Threshold {threshold_best:.2f}, pF1 Score: {val_max:.2f}')\n\n# Plot pF1 by Threshold\nplt.xticks(np.arange(0, 110, 10), [f'{t:.2f}' for t in np.arange(0, 1.1, 0.1)])\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.xlim(0, 100)\nplt.ylim(0, 1)\nplt.xlabel('Threshold')\nplt.ylabel('pF1 Score')\nplt.legend(fontsize=12)\nplt.grid()\nplt.show()\nprint(f'Best Threshold {threshold_best:.2f}.')\n\nprint(f'pF1 Score: {val_max:.2f}.')\n\n# Training History","metadata":{"execution":{"iopub.status.busy":"2023-02-15T06:42:34.561469Z","iopub.execute_input":"2023-02-15T06:42:34.561736Z","iopub.status.idle":"2023-02-15T06:42:34.574169Z","shell.execute_reply.started":"2023-02-15T06:42:34.561699Z","shell.execute_reply":"2023-02-15T06:42:34.573533Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Writing convext_m3.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python convext_m3.py","metadata":{"execution":{"iopub.status.busy":"2023-02-15T06:42:34.575119Z","iopub.execute_input":"2023-02-15T06:42:34.575294Z","iopub.status.idle":"2023-02-15T07:53:04.061975Z","shell.execute_reply.started":"2023-02-15T06:42:34.575272Z","shell.execute_reply":"2023-02-15T07:53:04.060352Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"2023-02-15 06:42:35.347140: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 06:42:35.347207: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTensorflow Version: 2.4.1\nPython Version: 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \n[GCC 9.3.0]\nCompute dtype: float32\nVariable dtype: float32\nRunning on TPU  grpc://10.0.0.2:8470\n2023-02-15 06:42:38.386954: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 06:42:38.387214: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 06:42:38.387243: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2023-02-15 06:42:38.387276: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (375a1fcc8acc): /proc/driver/nvidia/version does not exist\n2023-02-15 06:42:38.387783: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-15 06:42:38.388151: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 06:42:38.393458: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 06:42:38.393490: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30405}\n2023-02-15 06:42:38.408674: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 06:42:38.408782: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30405}\n2023-02-15 06:42:38.409433: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30405\nN_REPLICAS: 8, IS_TPU: True\nBATCH_SIZE: 64\n2023-02-15 06:42:43.288614: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\nFound 100 TFRecords\n2023-02-15 06:42:43.655791: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n# TFRECORDS_TRAIN: 83, # TFRECORDS_VAL: 20\nTRAIN_STEPS_PER_EPOCH: 710, VAL_STEPS_PER_EPOCH: 171\nimage shape: (64, 1344, 768, 1), y_batch shape: (64,)\nimage dtype: uint8, y_batch dtype: <dtype: 'int64'>\nimage min: 0.00, max: 255.00\nimage shape: (64, 1344, 768, 1), labels shape: (64,), image dtype: <dtype: 'uint8'>, labels dtype: <dtype: 'int64'>\nepoch 0 took: 0.91 sec, mean step duration: 91.0ms, images/s: 703\n2023-02-15 06:42:47.241951: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 1\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676443367.238560784\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 1\",\"grpc_status\":3}\nepoch 1 took: 0.77 sec, mean step duration: 76.8ms, images/s: 833\nepoch 2 took: 0.73 sec, mean step duration: 73.4ms, images/s: 871\nCompute dtype: float32\nVariable dtype: float32\n>>>> Load pretrained from: /root/.keras/models/convnext_v2_tiny_384_imagenet21k-ft1k.h5\n171/171 [==============================] - 57s 206ms/step - loss: 2.4155 - pF1: 0.0394 - f1_score: 0.0391 - precision: 0.0200 - recall: 0.9942 - auc: 0.5310 - binary_accuracy: 0.0201\nEpoch 1/10\nlearning rate: 4.00e-05, weight decay: 3.20e-07\n710/710 [==============================] - ETA: 0s - loss: 0.3621 - pF1: 0.3387 - f1_score: 0.5874 - precision: 0.8030 - recall: 0.4631 - auc: 0.7722 - binary_accuracy: 0.95852023-02-15 06:51:44.066038: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 2\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676443904.065934377\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 2\",\"grpc_status\":3}\n710/710 [==============================] - 464s 554ms/step - loss: 0.3621 - pF1: 0.3387 - f1_score: 0.5874 - precision: 0.8030 - recall: 0.4631 - auc: 0.7722 - binary_accuracy: 0.9585 - val_loss: 0.1190 - val_pF1: 0.0619 - val_f1_score: 0.0244 - val_precision: 0.3000 - val_recall: 0.0127 - val_auc: 0.7343 - val_binary_accuracy: 0.9781\nEpoch 2/10\nlearning rate: 3.90e-05, weight decay: 3.12e-07\n710/710 [==============================] - 394s 555ms/step - loss: 0.5508 - pF1: 0.4356 - f1_score: 0.6151 - precision: 0.6081 - recall: 0.6222 - auc: 0.8213 - binary_accuracy: 0.9279 - val_loss: 0.1466 - val_pF1: 0.0384 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5645 - val_binary_accuracy: 0.9784\nEpoch 3/10\nlearning rate: 3.62e-05, weight decay: 2.89e-07\n710/710 [==============================] - 394s 554ms/step - loss: 0.3326 - pF1: 0.3626 - f1_score: 0.5938 - precision: 0.7593 - recall: 0.4875 - auc: 0.8214 - binary_accuracy: 0.9572 - val_loss: 0.1193 - val_pF1: 0.1106 - val_f1_score: 0.2778 - val_precision: 0.5114 - val_recall: 0.1907 - val_auc: 0.7840 - val_binary_accuracy: 0.9786\nEpoch 4/10\nlearning rate: 3.18e-05, weight decay: 2.54e-07\n710/710 [==============================] - 394s 555ms/step - loss: 0.2517 - pF1: 0.5985 - f1_score: 0.7861 - precision: 0.8465 - recall: 0.7337 - auc: 0.9352 - binary_accuracy: 0.9629 - val_loss: 0.0916 - val_pF1: 0.1643 - val_f1_score: 0.2595 - val_precision: 0.5125 - val_recall: 0.1737 - val_auc: 0.8062 - val_binary_accuracy: 0.9786\nEpoch 5/10\nlearning rate: 2.62e-05, weight decay: 2.09e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.2126 - pF1: 0.6532 - f1_score: 0.8181 - precision: 0.8554 - recall: 0.7838 - auc: 0.9551 - binary_accuracy: 0.9675 - val_loss: 0.1502 - val_pF1: 0.1545 - val_f1_score: 0.3101 - val_precision: 0.2445 - val_recall: 0.4237 - val_auc: 0.8361 - val_binary_accuracy: 0.9593\nEpoch 6/10\nlearning rate: 2.00e-05, weight decay: 1.60e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.1706 - pF1: 0.6021 - f1_score: 0.7650 - precision: 0.7739 - recall: 0.7562 - auc: 0.9620 - binary_accuracy: 0.9704 - val_loss: 0.0990 - val_pF1: 0.2028 - val_f1_score: 0.3236 - val_precision: 0.3445 - val_recall: 0.3051 - val_auc: 0.8338 - val_binary_accuracy: 0.9725\nEpoch 7/10\nlearning rate: 1.38e-05, weight decay: 1.11e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.1363 - pF1: 0.7616 - f1_score: 0.8781 - precision: 0.8837 - recall: 0.8726 - auc: 0.9821 - binary_accuracy: 0.9774 - val_loss: 0.0966 - val_pF1: 0.2567 - val_f1_score: 0.3453 - val_precision: 0.3667 - val_recall: 0.3263 - val_auc: 0.8033 - val_binary_accuracy: 0.9733\nEpoch 8/10\nlearning rate: 8.24e-06, weight decay: 6.60e-08\n710/710 [==============================] - 393s 554ms/step - loss: 0.1089 - pF1: 0.7906 - f1_score: 0.8892 - precision: 0.8854 - recall: 0.8930 - auc: 0.9885 - binary_accuracy: 0.9806 - val_loss: 0.0982 - val_pF1: 0.2261 - val_f1_score: 0.3095 - val_precision: 0.3401 - val_recall: 0.2839 - val_auc: 0.8161 - val_binary_accuracy: 0.9727\nEpoch 9/10\nlearning rate: 3.82e-06, weight decay: 3.06e-08\n710/710 [==============================] - 393s 553ms/step - loss: 0.0864 - pF1: 0.7842 - f1_score: 0.8901 - precision: 0.8860 - recall: 0.8944 - auc: 0.9916 - binary_accuracy: 0.9846 - val_loss: 0.1026 - val_pF1: 0.2888 - val_f1_score: 0.3271 - val_precision: 0.3646 - val_recall: 0.2966 - val_auc: 0.7866 - val_binary_accuracy: 0.9737\nEpoch 10/10\nlearning rate: 9.79e-07, weight decay: 7.83e-09\n710/710 [==============================] - 393s 554ms/step - loss: 0.0812 - pF1: 0.8517 - f1_score: 0.9228 - precision: 0.9185 - recall: 0.9272 - auc: 0.9940 - binary_accuracy: 0.9856 - val_loss: 0.1001 - val_pF1: 0.2734 - val_f1_score: 0.3387 - val_precision: 0.3744 - val_recall: 0.3093 - val_auc: 0.8008 - val_binary_accuracy: 0.9739\n  0%|          | 0/171 [00:00<?, ?it/s]\n2023-02-15 07:52:55.517179: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 188134, Output num: 0\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676447575.517000023\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 188134, Output num: 0\",\"grpc_status\":3}\n  0%|          | 0/101 [00:00<?, ?it/s]\nconvext_m3.py:513: RuntimeWarning: invalid value encountered in long_scalars\n  c_precision = ctp / (ctp + cfp)\nBest Threshold 0.64.\npF1 Score: 0.35.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile convext_m4.py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom tqdm.notebook import tqdm\nfrom multiprocessing import cpu_count\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom keras_cv_attention_models import convnext\n\nimport os\nimport time\nimport pickle\nimport math\nimport random\nimport sys\nimport cv2\nimport gc\nimport datetime\n\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Python Version: {sys.version}')\n\n# Save Versions\n\nnow = datetime.datetime.now().strftime(\"%d-%b-%Y %H-%M-%S\")\nnp.save(now, np.array([now]))\n\n# Mixed Precision Policy\n\n# float32 or mixed_float16 (mixed precision: compute float16, variable float32)\n# TPU is fast enough and has enough memory to use float32\npolicy = tf.keras.mixed_precision.Policy('float32')\ntf.keras.mixed_precision.set_global_policy(policy)\n\nprint(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\nprint(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n\n# Matplotlib Config\n\n# MatplotLib Global Settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.labelsize'] = 18\nmpl.rcParams['axes.titlesize'] = 24\n\n# Config\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    IS_TPU = True\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    IS_TPU = False\n    STRATEGY = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nN_REPLICAS = STRATEGY.num_replicas_in_sync\nprint(f'N_REPLICAS: {N_REPLICAS}, IS_TPU: {IS_TPU}')\n\n#/kaggle/input/rsna-tfrecords-768x1344-dataset\n\n# For TPU's the dataset needs to be stored in Google Cloud\n# Retrieve the Google Cloud location of the dataset\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('rsna-tfrecords-768x1344-dataset2')\n\nSEED = 43\nDEBUG = False\nVERSION = 'convext_m4'\n# Image dimensions\nIMG_HEIGHT = 1344\nIMG_WIDTH = 768\nN_CHANNELS = 1\nINPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 1)\nN_SAMPLES_TFRECORDS = 548\n\n# Peak Learning Rate\nLR_MAX = 5e-6 * N_REPLICAS\nWD_RATIO = 0.008\n\nN_WARMUP_EPOCHS = 0\nN_EPOCHS = 10\n\n# Batch size\nBATCH_SIZE = 8 * N_REPLICAS\n\n# Is Interactive Flag and COrresponding Verbosity Method\nIS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\nVERBOSE = 1 if IS_INTERACTIVE else 2\n\n# Tensorflow AUTO flag\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(f'BATCH_SIZE: {BATCH_SIZE}')\n\n# Seed\n\n# Seed all random number generators\ndef seed_everything(seed=SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()\n\n# Train\n\n# Train DataFrame\ntrain = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n\n\n# Utility Functions\n\n# short Tensorflow randin integer function\ndef tf_rand_int(minval, maxval, dtype=tf.int64):\n    minval = tf.cast(minval, dtype)\n    maxval = tf.cast(maxval, dtype)\n    return tf.random.uniform(shape=(), minval=minval, maxval=maxval, dtype=dtype)\n\n# chance of 1 in k\ndef one_in(k):\n    return 0 == tf_rand_int(0, k)\n\n# Dataset\n\n# Function to benchmark the dataset\ndef benchmark_dataset(dataset, num_epochs=3, n_steps_per_epoch=10, bs=BATCH_SIZE):\n    start_time = time.perf_counter()\n    for epoch_num in range(num_epochs):\n        for idx, (inputs, labels) in enumerate(dataset.take(n_steps_per_epoch + 1)):\n            if idx == 0:\n                epoch_start = time.perf_counter()\n            elif idx == 1 and epoch_num == 0:\n                image = inputs['image']\n                print(f'image shape: {image.shape}, labels shape: {labels.shape}, image dtype: {image.dtype}, labels dtype: {labels.dtype}')\n            else:\n                pass\n        \n        epoch_t = time.perf_counter() - epoch_start\n        mean_step_t = round(epoch_t / n_steps_per_epoch * 1000, 1)\n        n_imgs_per_s = int(1 / (mean_step_t / 1000) * bs)\n        print(f'epoch {epoch_num} took: {round(epoch_t, 2)} sec, mean step duration: {mean_step_t}ms, images/s: {n_imgs_per_s}')\n\n# Plots a batch of images\ndef show_batch(dataset, n_rows=16, n_cols=4):\n    inputs, targets = next(iter(dataset))\n    images = inputs['image'].numpy().squeeze()\n    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols*4, n_rows*7))\n    for r in range(n_rows):\n        for c in range(n_cols):\n            idx = r * n_cols + c\n            # Image\n            img = images[idx]\n            axes[r, c].imshow(img)\n            # Target\n            target = targets[idx]\n            axes[r, c].set_title(f'target: {target}', fontsize=16, pad=5)\n        \n    plt.show()\n\n# Decodes the TFRecords\ndef decode_image(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64),\n        'patient_id': tf.io.FixedLenFeature([], tf.int64),\n    })\n    \n    # Decode PNG Image\n    image = tf.io.decode_jpeg(features['image'], channels=N_CHANNELS)\n    # Explicit reshape needed for TPU\n    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, N_CHANNELS])\n\n    target = features['target']\n    \n    return { 'image': image }, target\n\ndef augment_image(X, y):\n    image = X['image']\n    \n    # Random Brightness\n    image = tf.image.random_brightness(image, 0.10)\n    \n    # Random Contrast\n    image = tf.image.random_contrast(image, 0.90, 1.10)\n    \n    # Random JPEG Quality\n    image = tf.image.random_jpeg_quality(image, 75, 100)\n    \n    # Random crop image with maximum of 10%\n    ratio = tf.random.uniform([], 0.75, 1.00)\n    img_height_crop = tf.cast(ratio * IMG_HEIGHT, tf.int32)\n    img_width_crop = tf.cast(ratio * IMG_WIDTH, tf.int32)\n    # Random offset for crop\n    img_height_offset = tf_rand_int(0, IMG_HEIGHT - img_height_crop)\n    img_width_offset = 0\n    # Crop And Resize\n    image = tf.slice(image, [img_height_offset, img_width_offset, 0], [img_height_crop, img_width_crop, N_CHANNELS])\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.BILINEAR)\n    # Clip pixel values in range [0,255] to prevent underflow/overflow\n    image = tf.clip_by_value(image, 0, 255)\n    image = tf.cast(image, tf.uint8)\n    \n    return { 'image': image }, y\n\n# Undersample majority class (0/negative) by randomly dropping them\ndef undersample_majority(X, y):\n    # Filter 2/3 of negative samples to upsample positive samples by a factor 3\n    return y == 1 or tf.random.uniform([]) > 0.40\n\n# TFRecord file paths\nTFRECORDS_FILE_PATHS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH}/*.tfrecords'))\nprint(f'Found {len(TFRECORDS_FILE_PATHS)} TFRecords')\n\n# Train Test Split\nTFRECORDS_TRAIN, TFRECORDS_VAL = train_test_split(TFRECORDS_FILE_PATHS, train_size=0.80, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\nTFRECORDS_TRAIN2, TFRECORDS_VAL2 = train_test_split(TFRECORDS_TRAIN, train_size=0.75, random_state=SEED, shuffle=True)\n#print(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN2)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL2)}')\nTFRECORDS_TRAIN3, TFRECORDS_VAL3 = train_test_split(TFRECORDS_TRAIN2, train_size=0.67, random_state=SEED, shuffle=True)\nTFRECORDS_TRAIN4, TFRECORDS_VAL4 = train_test_split(TFRECORDS_TRAIN3, train_size=0.5, random_state=SEED, shuffle=True)\n\nTFRECORDS_TRAIN = [x  for x in TFRECORDS_FILE_PATHS if x not in TFRECORDS_TRAIN4]\nTFRECORDS_VAL = TFRECORDS_TRAIN4\nGCS_DS_PATH_POS = KaggleDatasets().get_gcs_path('rsna-extra-positive-tfrecord')\nTFRECORDS_FILE_PATHS_POS = sorted(tf.io.gfile.glob(f'{GCS_DS_PATH_POS}/*.tfrecords'))\n\nTFRECORDS_TRAIN = TFRECORDS_TRAIN+TFRECORDS_FILE_PATHS_POS\nprint(f'# TFRECORDS_TRAIN: {len(TFRECORDS_TRAIN)}, # TFRECORDS_VAL: {len(TFRECORDS_VAL)}')\ndef get_dataset(tfrecords, bs=BATCH_SIZE, val=False, debug=True):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    \n    # Initialize dataset with TFRecords\n    dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=AUTO, compression_type='GZIP')\n    \n    # Decode mapping\n    dataset = dataset.map(decode_image, num_parallel_calls=AUTO)\n\n    if not val:\n        dataset = dataset.filter(undersample_majority)\n        dataset = dataset.map(augment_image, num_parallel_calls=AUTO)\n        dataset = dataset.with_options(ignore_order)\n        if not debug:\n            dataset = dataset.shuffle(1024)\n        dataset = dataset.repeat()        \n\n    dataset = dataset.batch(bs, drop_remainder=not val)\n    dataset = dataset.prefetch(AUTO)\n    \n    return dataset\n\n# Get Train/Validation datasets\ntrain_dataset = get_dataset(TFRECORDS_TRAIN, val=False, debug=False)\nval_dataset = get_dataset(TFRECORDS_VAL, val=True, debug=False)\n\nTRAIN_STEPS_PER_EPOCH = len(TFRECORDS_TRAIN) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nVAL_STEPS_PER_EPOCH = len(TFRECORDS_VAL) * N_SAMPLES_TFRECORDS // BATCH_SIZE\nprint(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}, VAL_STEPS_PER_EPOCH: {VAL_STEPS_PER_EPOCH}')\n\n# Sanity check, image and label statistics\nX_batch, y_batch = next(iter(get_dataset(TFRECORDS_TRAIN, val=False)))\nimage = X_batch['image'].numpy()\nprint(f'image shape: {image.shape}, y_batch shape: {y_batch.shape}')\nprint(f'image dtype: {image.dtype}, y_batch dtype: {y_batch.dtype}')\nprint(f'image min: {image.min():.2f}, max: {image.max():.2f}')\n\n# Benchmark Dataset\nbenchmark_dataset(get_dataset(TFRECORDS_TRAIN, val=False))\n\n# Show what we will be training on\nshow_batch(get_dataset(TFRECORDS_TRAIN, bs=64, val=False))\n\n# Tensorflow custom metric is just a conventional class object\nclass pF1(tf.keras.metrics.Metric):\n    # Initialize properties\n    def __init__(self, name='pF1', **kwargs):\n        super(pF1, self).__init__(name=name, **kwargs)\n        self.tc = self.add_weight(name='tc', initializer='zeros')\n        self.tp = self.add_weight(name='tp', initializer='zeros')\n        self.fp = self.add_weight(name='fp', initializer='zeros')\n\n    # Update state called on each batch with true and predicted labels\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.tc.assign_add(tf.cast(tf.reduce_sum(y_true), tf.float32))\n        self.tp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 1])), tf.float32))\n        self.fp.assign_add(tf.cast(tf.reduce_sum((y_pred[y_true == 0])), tf.float32))\n\n    # Result function is called to obtain result which is printed in progress bar\n    def result(self):\n        if self.tc == 0 or (self.tp + self.fp) == 0:\n            return 0.0\n        else:\n            precision = self.tp / (self.tp + self.fp)\n            recall = self.tp / (self.tc)\n            return 2 * (precision * recall) / (precision + recall)\n\n    # Reset state is called after each epoch to start fresh each epoch\n    def reset_state(self):\n        self.tc.assign(0)\n        self.tp.assign(0)\n        self.fp.assign(0)\n\ndef normalize(image):\n    # Repeat channels to create 3 channel images required by pretrained ConvNextV2 models\n    image = tf.repeat(image, repeats=3, axis=3)\n    # Cast to float 32\n    image = tf.cast(image, tf.float32)\n    # Normalize with respect to ImageNet mean/std\n    image = tf.keras.applications.imagenet_utils.preprocess_input(image, mode='torch')\n\n    return image\n\ndef get_model():\n    # Verify Mixed Policy Settings\n    print(f'Compute dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\n    print(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')\n    \n    with STRATEGY.scope():\n        # Set seed for deterministic weights initialization\n        seed_everything()\n        \n        # Inputs, note the names are equal to the dictionary keys in the dataset\n        image = tf.keras.layers.Input(INPUT_SHAPE, name='image', dtype=tf.uint8)\n        \n        # Normalize Input\n        image_norm = normalize(image)\n\n        # CNN Prediction in range [0,1]\n        x = convnext.ConvNeXtV2Tiny(\n            input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n            pretrained='imagenet21k-ft1k',\n            num_classes=0,\n        )(image_norm)\n        \n        # Average Pooling BxHxWxC -> BxC\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        # Dropout to prevent Overfitting\n        x = tf.keras.layers.Dropout(0.30)(x)\n        # Output value between [0, 1] using Sigmoid function\n        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n        # We will use the famous AdamW optimizer for fast learning with weight decay\n        optimizer = tfa.optimizers.AdamW(learning_rate=LR_MAX, weight_decay=LR_MAX*WD_RATIO, epsilon=1e-6)\n\n        # Loss\n        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n        \n        # Metrics\n        metrics = [\n            pF1(),\n            tfa.metrics.F1Score(num_classes=1, threshold=0.50),\n            tf.keras.metrics.Precision(),\n            tf.keras.metrics.Recall(),\n            tf.keras.metrics.AUC(),\n            tf.keras.metrics.BinaryAccuracy(),\n        ]\n\n        model = tf.keras.models.Model(inputs=image, outputs=outputs)\n        \n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n        return model\n\n# Pretrained File Path: '/kaggle/input/sartorius-training-dataset/model.h5'\ntf.keras.backend.clear_session()\n# enable XLA optmizations\ntf.config.optimizer.set_jit(True)\n\nmodel = get_model()\n\n# Weight Initilization\n\n# Validation metric on initialized model\n_ = model.evaluate(\n        get_dataset(TFRECORDS_VAL, val=True),\n        verbose=VERBOSE,\n        steps=VAL_STEPS_PER_EPOCH,\n    )\n\n# Learning rate scheduler with logaritmic warmup and cosine decay\ndef lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n    \n    if current_step < num_warmup_steps:\n        return lr_max * 0.10 ** (num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n\n# Plot the learning rate scheduler\ndef plot_lr_schedule(lr_schedule, epochs):\n    fig = plt.figure(figsize=(20, 10))\n    plt.plot([None] + lr_schedule + [None])\n    # X Labels\n    x = np.arange(1, epochs + 1)\n    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n    plt.xlim([1, epochs])\n    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n    \n    # Increase y-limit for better readability\n    plt.ylim([0, max(lr_schedule) * 1.1])\n    \n    # Title\n    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n    \n    # Plot Learning Rates\n    for x, val in enumerate(lr_schedule):\n        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n            if x < len(lr_schedule) - 1:\n                if lr_schedule[x - 1] < val:\n                    ha = 'right'\n                else:\n                    ha = 'left'\n            elif x == 0:\n                ha = 'right'\n            else:\n                ha = 'left'\n            plt.plot(x + 1, val, 'o', color='black');\n            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n    \n    plt.xlabel('Epoch', size=16, labelpad=5)\n    plt.ylabel('Learning Rate', size=16, labelpad=5)\n    plt.grid()\n    plt.show()\n\n# Learning rate for encoder\nLR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\nplot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n\n# Learning Rate Callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n\n# Weight Decay Callback\n\n# Tensorflow Learning Rate Scheduler does not update weight decay, need to do it manually in a custom callback\nclass WeightDecayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, wd_ratio=WD_RATIO):\n        self.step_counter = 0\n        self.wd_ratio = wd_ratio\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')\n\n# Train model on TPU!\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=f'best_{VERSION}.h5',\n    save_weights_only=True,\n    monitor='val_pF1',\n    mode='max',\n    save_best_only=True)\nhistory = model.fit(\n        train_dataset,\n        steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n        validation_data = val_dataset,\n        epochs = N_EPOCHS,\n        verbose = VERBOSE,\n        callbacks = [\n            model_checkpoint_callback,\n            lr_callback,\n            WeightDecayCallback(),\n        ],\n        class_weight = {\n            0: 1.0,\n            1: 3.0,\n        },\n    )\n\n# Save model weights for inference\nmodel.save_weights(f'model_{VERSION}.h5')\n\n# F1 By Threshold\n\n# Get true labels and predictions for validation set\ny_true_val = []\ny_pred_val = []\nfor X_batch, y_batch in tqdm(get_dataset(TFRECORDS_VAL, val=True), total=VAL_STEPS_PER_EPOCH):\n    y_true_val += y_batch.numpy().tolist()\n    y_pred_val += model.predict_on_batch(X_batch).squeeze().tolist()\n\n# source: https://www.kaggle.com/code/sohier/probabilistic-f-score\n# Competition Leaderboard Metric\ndef pfbeta(labels, predictions, beta=1):\n    y_true_count = 0\n    ctp = 0\n    cfp = 0\n\n    for idx in range(len(labels)):\n        prediction = min(max(predictions[idx], 0), 1)\n        if (labels[idx]):\n            y_true_count += 1\n            ctp += prediction\n        else:\n            cfp += prediction\n\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp)\n    c_recall = ctp / y_true_count\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n        return result\n    else:\n        return 0\n\n# Plot pF1 by threshold plot to find best threshold\npf1_by_threshold = []\nthresholds = np.arange(0, 1.01, 0.01)\nfor t in tqdm(thresholds):\n    # Compute pF1 for each threshold\n    pf1_by_threshold.append(\n        pfbeta(y_true_val, y_pred_val > t)\n    )\n    \nplt.figure(figsize=(15,8))\nplt.title('F1 By Threshold', size=24)\nplt.plot(pf1_by_threshold, label='F1 Score')\n\n# Best threshold and pF1 score\narg_max = np.argmax(pf1_by_threshold)\nval_max = np.max(pf1_by_threshold)\nthreshold_best = thresholds[arg_max]\nplt.scatter(arg_max, val_max, color='red', label=f'Best Threshold {threshold_best:.2f}, pF1 Score: {val_max:.2f}')\n\n# Plot pF1 by Threshold\nplt.xticks(np.arange(0, 110, 10), [f'{t:.2f}' for t in np.arange(0, 1.1, 0.1)])\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.xlim(0, 100)\nplt.ylim(0, 1)\nplt.xlabel('Threshold')\nplt.ylabel('pF1 Score')\nplt.legend(fontsize=12)\nplt.grid()\nplt.show()\nprint(f'Best Threshold {threshold_best:.2f}.')\n\nprint(f'pF1 Score: {val_max:.2f}.')\n\n# Training History","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:53:04.068424Z","iopub.execute_input":"2023-02-15T07:53:04.068802Z","iopub.status.idle":"2023-02-15T07:53:04.085831Z","shell.execute_reply.started":"2023-02-15T07:53:04.068742Z","shell.execute_reply":"2023-02-15T07:53:04.084694Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Writing convext_m4.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python convext_m4.py","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:53:04.087343Z","iopub.execute_input":"2023-02-15T07:53:04.087607Z","iopub.status.idle":"2023-02-15T09:03:28.021266Z","shell.execute_reply.started":"2023-02-15T07:53:04.087581Z","shell.execute_reply":"2023-02-15T09:03:28.018630Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"2023-02-15 07:53:04.889634: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 07:53:04.889693: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTensorflow Version: 2.4.1\nPython Version: 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \n[GCC 9.3.0]\nCompute dtype: float32\nVariable dtype: float32\nRunning on TPU  grpc://10.0.0.2:8470\n2023-02-15 07:53:08.070269: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 07:53:08.070695: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-15 07:53:08.070730: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2023-02-15 07:53:08.070760: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (375a1fcc8acc): /proc/driver/nvidia/version does not exist\n2023-02-15 07:53:08.071387: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-15 07:53:08.071726: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 07:53:08.072898: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-15 07:53:08.078743: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 07:53:08.078806: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30523}\n2023-02-15 07:53:08.093857: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-15 07:53:08.093922: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30523}\n2023-02-15 07:53:08.094469: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30523\nN_REPLICAS: 8, IS_TPU: True\nBATCH_SIZE: 64\n2023-02-15 07:53:13.683549: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\nFound 100 TFRecords\n2023-02-15 07:53:14.019779: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n# TFRECORDS_TRAIN: 83, # TFRECORDS_VAL: 20\nTRAIN_STEPS_PER_EPOCH: 710, VAL_STEPS_PER_EPOCH: 171\nimage shape: (64, 1344, 768, 1), y_batch shape: (64,)\nimage dtype: uint8, y_batch dtype: <dtype: 'int64'>\nimage min: 0.00, max: 255.00\nimage shape: (64, 1344, 768, 1), labels shape: (64,), image dtype: <dtype: 'uint8'>, labels dtype: <dtype: 'int64'>\nepoch 0 took: 0.73 sec, mean step duration: 72.8ms, images/s: 879\n2023-02-15 07:53:17.097199: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 0\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676447597.093898739\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 97, Output num: 0\",\"grpc_status\":3}\nepoch 1 took: 0.75 sec, mean step duration: 75.3ms, images/s: 849\nepoch 2 took: 0.79 sec, mean step duration: 79.0ms, images/s: 810\nCompute dtype: float32\nVariable dtype: float32\n>>>> Load pretrained from: /root/.keras/models/convnext_v2_tiny_384_imagenet21k-ft1k.h5\n171/171 [==============================] - 56s 206ms/step - loss: 4.0189 - pF1: 0.0316 - f1_score: 0.0317 - precision: 0.0161 - recall: 1.0000 - auc: 0.4285 - binary_accuracy: 0.0161\nEpoch 1/10\nlearning rate: 4.00e-05, weight decay: 3.20e-07\n710/710 [==============================] - ETA: 0s - loss: 0.4012 - pF1: 0.3056 - f1_score: 0.5295 - precision: 0.6622 - recall: 0.4410 - auc: 0.7403 - binary_accuracy: 0.94942023-02-15 08:02:10.579637: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 3\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676448130.579428546\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 36268, Output num: 3\",\"grpc_status\":3}\n710/710 [==============================] - 462s 553ms/step - loss: 0.4012 - pF1: 0.3056 - f1_score: 0.5295 - precision: 0.6622 - recall: 0.4410 - auc: 0.7403 - binary_accuracy: 0.9494 - val_loss: 0.1356 - val_pF1: 0.0362 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6274 - val_binary_accuracy: 0.9809\nEpoch 2/10\nlearning rate: 3.90e-05, weight decay: 3.12e-07\n710/710 [==============================] - 394s 555ms/step - loss: 0.3540 - pF1: 0.4960 - f1_score: 0.7465 - precision: 0.9141 - recall: 0.6308 - auc: 0.8468 - binary_accuracy: 0.9598 - val_loss: 0.1568 - val_pF1: 0.0515 - val_f1_score: 0.0165 - val_precision: 0.0588 - val_recall: 0.0096 - val_auc: 0.7611 - val_binary_accuracy: 0.9782\nEpoch 3/10\nlearning rate: 3.62e-05, weight decay: 2.89e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.3057 - pF1: 0.3971 - f1_score: 0.6355 - precision: 0.8093 - recall: 0.5232 - auc: 0.8535 - binary_accuracy: 0.9610 - val_loss: 0.1558 - val_pF1: 0.1119 - val_f1_score: 0.3167 - val_precision: 0.2897 - val_recall: 0.3493 - val_auc: 0.8628 - val_binary_accuracy: 0.9712\nEpoch 4/10\nlearning rate: 3.18e-05, weight decay: 2.54e-07\n710/710 [==============================] - 392s 553ms/step - loss: 0.2475 - pF1: 0.6134 - f1_score: 0.7980 - precision: 0.8632 - recall: 0.7420 - auc: 0.9355 - binary_accuracy: 0.9647 - val_loss: 0.0839 - val_pF1: 0.1700 - val_f1_score: 0.3273 - val_precision: 0.4463 - val_recall: 0.2584 - val_auc: 0.8653 - val_binary_accuracy: 0.9797\nEpoch 5/10\nlearning rate: 2.62e-05, weight decay: 2.09e-07\n710/710 [==============================] - 393s 553ms/step - loss: 0.2073 - pF1: 0.6633 - f1_score: 0.8181 - precision: 0.8499 - recall: 0.7887 - auc: 0.9578 - binary_accuracy: 0.9671 - val_loss: 0.1343 - val_pF1: 0.1677 - val_f1_score: 0.2760 - val_precision: 0.2041 - val_recall: 0.4258 - val_auc: 0.8573 - val_binary_accuracy: 0.9573\nEpoch 6/10\nlearning rate: 2.00e-05, weight decay: 1.60e-07\n710/710 [==============================] - 393s 553ms/step - loss: 0.1659 - pF1: 0.6174 - f1_score: 0.7792 - precision: 0.7913 - recall: 0.7675 - auc: 0.9632 - binary_accuracy: 0.9720 - val_loss: 0.1148 - val_pF1: 0.1915 - val_f1_score: 0.3236 - val_precision: 0.2527 - val_recall: 0.4498 - val_auc: 0.8794 - val_binary_accuracy: 0.9641\nEpoch 7/10\nlearning rate: 1.38e-05, weight decay: 1.11e-07\n710/710 [==============================] - 393s 554ms/step - loss: 0.1250 - pF1: 0.7780 - f1_score: 0.8859 - precision: 0.8875 - recall: 0.8842 - auc: 0.9859 - binary_accuracy: 0.9784 - val_loss: 0.0831 - val_pF1: 0.2455 - val_f1_score: 0.3499 - val_precision: 0.3851 - val_recall: 0.3206 - val_auc: 0.8575 - val_binary_accuracy: 0.9772\nEpoch 8/10\nlearning rate: 8.24e-06, weight decay: 6.60e-08\n710/710 [==============================] - 392s 553ms/step - loss: 0.0974 - pF1: 0.7960 - f1_score: 0.8930 - precision: 0.8906 - recall: 0.8954 - auc: 0.9908 - binary_accuracy: 0.9826 - val_loss: 0.1118 - val_pF1: 0.1928 - val_f1_score: 0.2510 - val_precision: 0.2104 - val_recall: 0.3110 - val_auc: 0.8324 - val_binary_accuracy: 0.9645\nEpoch 9/10\nlearning rate: 3.82e-06, weight decay: 3.06e-08\n710/710 [==============================] - 392s 553ms/step - loss: 0.0783 - pF1: 0.8217 - f1_score: 0.9104 - precision: 0.9086 - recall: 0.9123 - auc: 0.9934 - binary_accuracy: 0.9862 - val_loss: 0.0983 - val_pF1: 0.2625 - val_f1_score: 0.3231 - val_precision: 0.2972 - val_recall: 0.3541 - val_auc: 0.8534 - val_binary_accuracy: 0.9717\nEpoch 10/10\nlearning rate: 9.79e-07, weight decay: 7.83e-09\n710/710 [==============================] - 393s 554ms/step - loss: 0.0680 - pF1: 0.8652 - f1_score: 0.9356 - precision: 0.9334 - recall: 0.9377 - auc: 0.9960 - binary_accuracy: 0.9879 - val_loss: 0.0924 - val_pF1: 0.2580 - val_f1_score: 0.3077 - val_precision: 0.3315 - val_recall: 0.2871 - val_auc: 0.8373 - val_binary_accuracy: 0.9753\n  0%|          | 0/171 [00:00<?, ?it/s]\n2023-02-15 09:03:19.448797: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 188926, Output num: 0\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676451799.448234488\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 188926, Output num: 0\",\"grpc_status\":3}\n  0%|          | 0/101 [00:00<?, ?it/s]\nconvext_m4.py:513: RuntimeWarning: invalid value encountered in long_scalars\n  c_precision = ctp / (ctp + cfp)\nBest Threshold 0.71.\npF1 Score: 0.33.\n","output_type":"stream"}]}]}